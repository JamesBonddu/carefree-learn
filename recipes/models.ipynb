{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Models Recipes\n",
    "\n",
    "In this page, we will show you how to customize your own models. In `carefree-learn`, it is fairly easy to define various kinds of models with three APIs: `register_ml_module` (for [ML models](#ML-Models)), `register_module` (for [Other Models](#Other-Models)) and `register_custom_module` (for [Complex Models](#Complex-Models)).\n",
    "\n",
    "> You might notice that if you run the blocks with `register_*` calls for more than once, `carefree-learn` will throw a warning which says \" '...' has already been registered \", and your changes will have no effect. This is intentional because normally we **DO NOT** want to register anything for more than once.\n",
    "> \n",
    "> However, if you are using some interactive developing tools (e.g. Jupyter Notebook), it is very common to modify the implementations for more than once. In this case, we can set `allow_duplicate=True` in the `register_*` functions to bypass this check. And of course, this should **NEVER** happen in production for safety!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "\n",
    "- [One-Stage Models](#One-Stage-Models)\n",
    "  - [ML Models](#ML-Models)\n",
    "    - [Configurations](#Configurations)\n",
    "  - [Other Models](#Other-Models)\n",
    "- [Complex Models](#Complex-Models)\n",
    "  - [Simple Complex Models](#Simple-Complex-Models)\n",
    "    - [`__init__` / `forward`](#__init__-/-forward)\n",
    "    - [`g_parameters` / `d_parameters`](#g_parameters-/-d_parameters)\n",
    "    - [`train_step`](#train_step)\n",
    "    - [`evaluate_step`](#evaluate_step)\n",
    "  - [Full set of Functionalities](#Full-set-of-Functionalities)\n",
    "- [Appendix](#Appendix)\n",
    "  - [ML Encodings](#ML-Encodings)\n",
    "    - [Optimizations](#Optimizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You might also notice that:\n",
    "> - The class name defined below somehow matches the registered name. This is also not required, since `carefree-learn` only cares about the name that you pass to the `register_*` function, and will not check the actual class name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cflearn\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Stage Models\n",
    "\n",
    "We will first jump into the typical situation where we need to define one-stage models. The 'one-stage' here means that the training step only contains one optimizer step, so we can focus on how to define the forward pass of our models, and leave `carefree-learn` to handle other stuffs.\n",
    "\n",
    "> The contrary of 'one-stage' models will be the [Complex Models](#Complex-Models). A typical 'complex' model is the `GAN` models, which in general should perform a generator optimizing step **AND** a discriminator optimizing step in one **SINGLE** training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `carefree-learn`, Machine Learning Models will be slightly different to other models because:\n",
    "- We have integrated some common data-preprocessing methods into the ML pipeline (e.g. `one_hot` encoding, `embedding`).\n",
    "- There are some shared arguments that should be used by all ML models: input dimension, output dimension and number of history steps (this is used in timeseries tasks).\n",
    "\n",
    "Therefore, `carefree-learn` has:\n",
    "- Wrapped the registered `nn.Module` internally to make it suitable for ML pipeline.\n",
    "- Introduced three (optional) pre-defined arguments for all ML Models: `input_dim`, `output_dim` and `num_history`.\n",
    "  - We call it the 'dimension system' of `carefree-learn`.\n",
    "\n",
    "We will dive into these details in the following sections step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cflearn.register_ml_module(\"my_linear0\", allow_duplicate=False)\n",
    "class MyLinear0(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, net: Tensor) -> Tensor:\n",
    "        return self.net(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We can now integrate them into our ML pipeline with the `fit_ml` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    MyLinear0                                [-1, 5]                                  [-1, 2]                   12\n",
      "      Linear                                 [-1, 5]                                  [-1, 2]                   12\n",
      "========================================================================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-25_01-57-53-207709\\checkpoints\n",
      "| epoch  -1  [-1 / 1] [0.026s] | acc : 0.430000 | auc : 0.360795 | score : 0.395397 |\n"
     ]
    }
   ],
   "source": [
    "n       = 100\n",
    "in_dim  = 5\n",
    "out_dim = 2\n",
    "\n",
    "x = np.random.random([n, in_dim])\n",
    "y = np.random.randint(0, 2, [n, 1])\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    x,\n",
    "    y,\n",
    "    core_name=\"my_linear0\",\n",
    "    output_dim=out_dim,\n",
    "    is_classification=True,\n",
    "    # debug setting, indicating that we only train for one step\n",
    "    fixed_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `output_dim` passed to `fit_ml` will be passed into your model as well.\n",
    "- The `input_dim` is not provided, and `carefree-learn` will use `x.shape[1]` as `input_dim`.\n",
    "\n",
    "If the `input_dim` is specified, we will use it regardless of `x.shape[1]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    MyLinear1                                [-1, 5]                                  [-1, 2]                   22\n",
      "      Linear                                [-1, 10]                                  [-1, 2]                   22\n",
      "========================================================================================================================\n",
      "Total params: 22\n",
      "Trainable params: 22\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-25_01-57-53-282090\\checkpoints\n",
      "| epoch  -1  [-1 / 1] [0.018s] | acc : 0.470000 | auc : 0.492694 | score : 0.481347 |\n"
     ]
    }
   ],
   "source": [
    "@cflearn.register_ml_module(\"my_linear1\", allow_duplicate=False)\n",
    "class MyLinear1(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, net: Tensor) -> Tensor:\n",
    "        # duplicate the input\n",
    "        return self.net(torch.cat([net, net], dim=1))\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    x,\n",
    "    y,\n",
    "    core_name=\"my_linear1\",\n",
    "    # the input is duplicated, so we need to specify the `input_dim`\n",
    "    input_dim=5 * 2,\n",
    "    output_dim=out_dim,\n",
    "    is_classification=True,\n",
    "    # debug setting, indicating that we only train for one step\n",
    "    fixed_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that in the 'summary' panel shown above, the `MyLinear1` module is 'wrapped' by `MLModel` and `_`. This is what `carefree-learn` does internally to make your model compatible for the ML pipeline.\n",
    "\n",
    "Here's an example, with `one_hot` encoding and `embedding` considered, to show you why this kind of 'wrapping' is useful and powerful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cflearn import MERGED_KEY\n",
    "from cflearn import ONE_HOT_KEY\n",
    "from cflearn import EMBEDDING_KEY\n",
    "from cflearn import NUMERICAL_KEY\n",
    "\n",
    "@cflearn.register_ml_module(\"my_linear2\", allow_duplicate=False)\n",
    "class MyLinear2(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        print(\"> input_dim\", input_dim)\n",
    "        self.net = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    # notice that we use `batch` here, and the naming is important!\n",
    "    def forward(self, batch: Dict[str, Tensor]) -> Tensor:\n",
    "        merged = batch[MERGED_KEY]\n",
    "        one_hot = batch[ONE_HOT_KEY]\n",
    "        embedding = batch[EMBEDDING_KEY]\n",
    "        numerical = batch[NUMERICAL_KEY]\n",
    "        print()\n",
    "        print(\">>> merged\", merged.shape)\n",
    "        if one_hot is not None:\n",
    "            print(\">>> one_hot\", one_hot.shape)\n",
    "        if embedding is not None:\n",
    "            print(\">>> embedding\", embedding.shape)\n",
    "        if numerical is not None:\n",
    "            print(\">>> numerical\", numerical.shape)\n",
    "        print()\n",
    "        return self.net(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As the comment says, the naming of the `forward` argument, `batch`, is important! Because `carefree-learn` will then know that you require the full batch, instead of a single `Tensor`.\n",
    "\n",
    "The newly defined `my_linear1` model can be used as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> input_dim 5\n",
      "\n",
      ">>> merged torch.Size([1, 5])\n",
      ">>> numerical torch.Size([1, 5])\n",
      "\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    MyLinear2                                                                                                     \n",
      "      Linear                                 [-1, 5]                                  [-1, 2]                   12\n",
      "========================================================================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "\n",
      ">>> merged torch.Size([100, 5])\n",
      ">>> numerical torch.Size([100, 5])\n",
      "\n",
      "\n",
      ">>> merged torch.Size([100, 5])\n",
      ">>> numerical torch.Size([100, 5])\n",
      "\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-25_01-57-53-363167\\checkpoints\n",
      "\n",
      ">>> merged torch.Size([100, 5])\n",
      ">>> numerical torch.Size([100, 5])\n",
      "\n",
      "| epoch  -1  [-1 / 1] [0.018s] | acc : 0.550000 | auc : 0.573051 | score : 0.561525 |\n"
     ]
    }
   ],
   "source": [
    "m = cflearn.api.fit_ml(\n",
    "    x,\n",
    "    y,\n",
    "    core_name=\"my_linear2\",\n",
    "    output_dim=out_dim,\n",
    "    is_classification=True,\n",
    "    # debug setting, indicating that we only train for one step\n",
    "    fixed_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the powerful part is that, it can now utilize the encoding methods (`one_hot` / `embedding`) provided by `carefree-learn`:\n",
    "\n",
    "> We will use some encoding settings in a few following blocks. Please refer to the [Appendix](#ML-Encodings) section for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> input_dim 26\n",
      "\n",
      ">>> merged torch.Size([1, 26])\n",
      ">>> one_hot torch.Size([1, 13])\n",
      ">>> embedding torch.Size([1, 8])\n",
      ">>> numerical torch.Size([1, 5])\n",
      "\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  Encoder                                    [-1, 8]                      [[-1, 13], [-1, 8]]                   56\n",
      "    ModuleDict-1                                                                                                  \n",
      "      OneHot                                    [-1]                                 [-1, 13]                    0\n",
      "        Lambda                                  [-1]                                 [-1, 13]                    0\n",
      "    ModuleDict-0                                                                                                  \n",
      "      Embedding                              [-1, 2]                               [-1, 2, 4]                   56\n",
      "        Lambda                               [-1, 2]                               [-1, 2, 4]                    0\n",
      "    Dropout                                  [-1, 8]                                  [-1, 8]                    0\n",
      "  _                                                                                                               \n",
      "    MyLinear2                                                                                                     \n",
      "      Linear                                [-1, 26]                                  [-1, 2]                   54\n",
      "========================================================================================================================\n",
      "Total params: 110\n",
      "Trainable params: 110\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "\n",
      ">>> merged torch.Size([100, 26])\n",
      ">>> one_hot torch.Size([100, 13])\n",
      ">>> embedding torch.Size([100, 8])\n",
      ">>> numerical torch.Size([100, 5])\n",
      "\n",
      "\n",
      ">>> merged torch.Size([100, 26])\n",
      ">>> one_hot torch.Size([100, 13])\n",
      ">>> embedding torch.Size([100, 8])\n",
      ">>> numerical torch.Size([100, 5])\n",
      "\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-25_01-57-53-431266\\checkpoints\n",
      "\n",
      ">>> merged torch.Size([100, 26])\n",
      ">>> one_hot torch.Size([100, 13])\n",
      ">>> embedding torch.Size([100, 8])\n",
      ">>> numerical torch.Size([100, 5])\n",
      "\n",
      "| epoch  -1  [-1 / 1] [0.026s] | acc : 0.380000 | auc : 0.398925 | score : 0.389462 |\n"
     ]
    }
   ],
   "source": [
    "n                 = 100\n",
    "in_dim            = 5\n",
    "one_hot_dim       = 13\n",
    "embedding_dim     = 7\n",
    "out_dim           = 2\n",
    "# some encoding settings. Please refer to the `ML Encodings` section in the `Appendix` section for more details.\n",
    "one_hot_setting   = dict(dim=one_hot_dim, methods=\"one_hot\")\n",
    "embedding_setting = dict(dim=embedding_dim, methods=\"embedding\")\n",
    "encoding_settings = {\n",
    "    # one hot columns   : [6]\n",
    "    6: one_hot_setting,\n",
    "    # embedding columns : [5, 7] \n",
    "    5: embedding_setting,\n",
    "    7: embedding_setting,\n",
    "}\n",
    "\n",
    "x = np.hstack([\n",
    "    np.random.random([n, in_dim]),\n",
    "    np.random.randint(0, embedding_dim, [n, 1]),\n",
    "    np.random.randint(0, one_hot_dim, [n, 1]),\n",
    "    np.random.randint(0, embedding_dim, [n, 1]),\n",
    "])\n",
    "y = np.random.randint(0, 2, [n, 1])\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    x,\n",
    "    y,\n",
    "    core_name=\"my_linear2\",\n",
    "    output_dim=out_dim,\n",
    "    is_classification=True,\n",
    "    # encoding settings\n",
    "    encoding_settings=encoding_settings,\n",
    "    # debug setting, indicating that we only train for one step\n",
    "    fixed_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We don't need to specify the `input_dim`. In this case, `carefree-learn` will use `merged_dim` as `input_dim`.\n",
    "- `merged_dim` = `one_hot_dim` + `embedding_dim` + `numerical_dim`, because `carefree-learn` will simply concat every kind of inputs together to create the `merged` input.\n",
    "- In the 'summary' panel, we can find that the `Embedding` module output `4` dimension `Tensor` with `56` trainable params. That's because we have `2` columns for embedding, each has `7` different values, so `56 = 2 * 7 * 4`.\n",
    "\n",
    "Although it is already very powerful to have access to every part of the inputs, it is still pretty hard to utilize them, because currently we can only get the `merged_dim` in our `__init__` method. `carefree-learn` therefore provides a `dimensions` argument that gives you all you want.\n",
    "\n",
    "For example, let's implement the famous [Wide & Deep](https://arxiv.org/pdf/1606.07792.pdf)-like model, which feeds the `one_hot` part to a `Linear` model, and feeds the `numerical` and `embedding` part to a `MLP` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cflearn.register_ml_module(\"my_wide_and_deep\", allow_duplicate=True)\n",
    "class MyWideAndDeep(nn.Module):\n",
    "    # notice that we use `dimensions` here, and the naming is important!\n",
    "    def __init__(self, dimensions, output_dim):\n",
    "        super().__init__()\n",
    "        print(\">\", dimensions)\n",
    "        self.wide = nn.Linear(dimensions.one_hot_dim, output_dim)\n",
    "        self.deep = nn.Sequential(\n",
    "            nn.Linear(dimensions.embedding_dim + dimensions.numerical_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch: Dict[str, Tensor]) -> Tensor:\n",
    "        one_hot = batch[ONE_HOT_KEY]\n",
    "        embedding = batch[EMBEDDING_KEY]\n",
    "        numerical = batch[NUMERICAL_KEY]\n",
    "        wide_output = self.wide(one_hot)\n",
    "        deep_output = self.deep(torch.cat([embedding, numerical], dim=1))\n",
    "        return wide_output + deep_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As the comment says, the naming of the `__init__` argument, `dimensions`, is important! Because `carefree-learn` will then know that you require the full `dimensions` information, instead of a single `input_dim`.\n",
    "\n",
    "We can run it and see if it works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Dimensions(\n",
      "    merged_dim    = 26\n",
      "    one_hot_dim   = 13\n",
      "    embedding_dim = 8\n",
      "    numerical_dim = 5\n",
      ")\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  Encoder                                    [-1, 8]                      [[-1, 13], [-1, 8]]                   56\n",
      "    ModuleDict-1                                                                                                  \n",
      "      OneHot                                    [-1]                                 [-1, 13]                    0\n",
      "        Lambda                                  [-1]                                 [-1, 13]                    0\n",
      "    ModuleDict-0                                                                                                  \n",
      "      Embedding                              [-1, 2]                               [-1, 2, 4]                   56\n",
      "        Lambda                               [-1, 2]                               [-1, 2, 4]                    0\n",
      "    Dropout                                  [-1, 8]                                  [-1, 8]                    0\n",
      "  _                                                                                                               \n",
      "    MyWideAndDeep                                                                                                 \n",
      "      Linear                                [-1, 13]                                  [-1, 2]                   28\n",
      "      Sequential                            [-1, 13]                                  [-1, 2]                2,050\n",
      "        Linear-0                            [-1, 13]                                [-1, 128]                1,792\n",
      "        ReLU                               [-1, 128]                                [-1, 128]                    0\n",
      "        Linear-1                           [-1, 128]                                  [-1, 2]                  258\n",
      "========================================================================================================================\n",
      "Total params: 2,134\n",
      "Trainable params: 2,134\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.01\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-25_01-57-53-513197\\checkpoints\n",
      "| epoch  -1  [-1 / 1] [0.024s] | acc : 0.580000 | auc : 0.507647 | score : 0.543823 |\n"
     ]
    }
   ],
   "source": [
    "m = cflearn.api.fit_ml(\n",
    "    x,\n",
    "    y,\n",
    "    core_name=\"my_wide_and_deep\",\n",
    "    output_dim=out_dim,\n",
    "    is_classification=True,\n",
    "    # encoding settings\n",
    "    encoding_settings=encoding_settings,\n",
    "    # debug setting, indicating that we only train for one step\n",
    "    fixed_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bravo! Everything works like a charm! 🥳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Configurations\n",
    "\n",
    "So far we've introduced the 'dimension system' of the ML Models in `carefree-learn`, but you might want to know how to use custom hyper-parameters in your own models. For example, to specify `use_bias` in `my_linear`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cflearn.register_ml_module(\"my_linear3\", allow_duplicate=False)\n",
    "class MyLinear3(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, *, use_bias: bool):\n",
    "        super().__init__()\n",
    "        self.net = nn.Linear(input_dim, output_dim, bias=use_bias)\n",
    "    \n",
    "    def forward(self, net):\n",
    "        return self.net(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use `my_linear3` directly without specifying configurations, `carefree-learn` will throw an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__() missing 1 required keyword-only argument: 'use_bias'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    m = cflearn.api.fit_ml(\n",
    "        x,\n",
    "        y,\n",
    "        core_name=\"my_linear3\",\n",
    "        output_dim=out_dim,\n",
    "        is_classification=True,\n",
    "        # debug setting, indicating that we only train for one step\n",
    "        fixed_steps=1,\n",
    "    )\n",
    "except TypeError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the error indicates, we are missing `use_bias` to initialize the `MyLinear3` module. To fix it, we can add a `core_config` part to the `fit_ml` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    MyLinear3                                [-1, 8]                                  [-1, 2]                   16\n",
      "      Linear                                 [-1, 8]                                  [-1, 2]                   16\n",
      "========================================================================================================================\n",
      "Total params: 16\n",
      "Trainable params: 16\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-25_01-57-53-631876\\checkpoints\n",
      "| epoch  -1  [-1 / 1] [0.026s] | acc : 0.420000 | auc : 0.446052 | score : 0.433026 |\n"
     ]
    }
   ],
   "source": [
    "m = cflearn.api.fit_ml(\n",
    "    x,\n",
    "    y,\n",
    "    core_name=\"my_linear3\",\n",
    "    # Add This!\n",
    "    core_config=dict(use_bias=False),\n",
    "    output_dim=out_dim,\n",
    "    is_classification=True,\n",
    "    # debug setting, indicating that we only train for one step\n",
    "    fixed_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the trainable parameters of `Linear` is only `16`, which means the `bias` has indeed been set to `False`.\n",
    "\n",
    "> We put `use_bias` after a `*` to make it a keyword-only argument. It is not forced to do so, but in general it's recommended because:\n",
    "> - It will make your module easier to understand when it is used by others.\n",
    "> - It can separate the `carefree-learn`'s 'dimension system' from your own custom configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models\n",
    "\n",
    "Besides ML Models, customizing other models (e.g. CV models) with `register_module` API is almost the same as writing custom `nn.Module`. For example, let's build a simple image classification model from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cflearn.register_module(\"my_image_classifier\", allow_duplicate=True)\n",
    "class MyImageClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, img_size, output_dim):\n",
    "        super().__init__()\n",
    "        flat_dim = in_channels * img_size ** 2\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(flat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, net):\n",
    "        # flatten the input first\n",
    "        net = net.view(net.shape[0], -1)\n",
    "        return self.net(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike ML Models, for any other models, `carefree-learn` will not 'inject' any pre-defined arguments to your `nn.Module`, so it will be safe & clean! 😉\n",
    "\n",
    "> In fact, even for ML Models, you can ignore the 'dimension system' completely! Just avoid using names like `in_dim` / `input_dim` / `out_dim` / `output_dim` and everything will be fine.\n",
    "\n",
    "We can play around with the `my_image_classifier` model on the famous `MNIST` dataset with `fit_cv` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "_                                                                                                                 \n",
      "  MyImageClassifier                  [-1, 1, 28, 28]                                 [-1, 10]              101,770\n",
      "    Sequential                             [-1, 784]                                 [-1, 10]              101,770\n",
      "      Linear-0                             [-1, 784]                                [-1, 128]              100,480\n",
      "      ReLU                                 [-1, 128]                                [-1, 128]                    0\n",
      "      Linear-1                             [-1, 128]                                 [-1, 10]                1,290\n",
      "========================================================================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.39\n",
      "Estimated Total Size (MB): 0.39\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-25_01-57-53-780476\\checkpoints\n",
      "| epoch  -1  [   -1 / 30000] [0.043s] | acc : 0.000000 | score : 0.000000 |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cflearn.api.cv.pipeline.CarefreePipeline at 0x256d9492d30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get MNIST data with `cflearn`'s predefined API\n",
    "data = cflearn.cv.MNISTData(batch_size=2, transform=\"to_tensor\")\n",
    "# use `fit_cv` API for training\n",
    "cflearn.api.fit_cv(\n",
    "    # This first argument passed to `fit_cv` is complicated and hard to explain briefly\n",
    "    # So we will cover its details in another article (the `Data Recipes`)\n",
    "    data,\n",
    "    # this is the name of your model\n",
    "    model_name=\"my_image_classifier\",\n",
    "    # these (and only these) settings will go into your model's __init__ method\n",
    "    model_config={\"in_channels\": 1, \"img_size\": 28, \"output_dim\": 10},\n",
    "    # these are some training settings\n",
    "    loss_name=\"cross_entropy\",\n",
    "    metric_names=\"acc\",\n",
    "    # debug setting, indicating that we only use a small portion of data to do validation\n",
    "    valid_portion=1.0e-5,\n",
    "    # debug setting, indicating that we only train for one step\n",
    "    fixed_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that the naming is slightly different from ML Models:\n",
    "- `core_name` -> `model_name`\n",
    "- `core_config` -> `model_config`\n",
    "\n",
    "This is because ML Models will 'wrap' customized models under the `MLModel`, which means they serve as the `core` of `MLModel`. That's why we use `core_name` & `core_config`. But for other situations, the customized models will be left as-is, so we use `model_name` & `model_config`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Models\n",
    "\n",
    "For complicated tasks where a 'single-stage' model cannot satisfy, we will need to define 'complex' models in which you have full control of the training step and the evaluation step.\n",
    "\n",
    "With the help of `inspect`, `carefree-learn` will let you **CHOOSE** the functionalities you need. So you can not only define a 'complex' model easily, but can also leverage a full set of powerful mechanisms such as [`amp`](https://pytorch.org/docs/stable/amp.html), gradient norm clipping, learning rate schedulings and so on.\n",
    "\n",
    "In the following sections, we will use the famous `GAN` model to illustrate the concepts. We will start from a simple implementation, and then gradually increase the complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cflearn.protocol import StepOutputs\n",
    "from cflearn.protocol import MetricsOutputs\n",
    "from cflearn.constants import INPUT_KEY\n",
    "from cflearn.constants import PREDICTIONS_KEY\n",
    "from cflearn.misc.toolkit import to_device\n",
    "from cflearn.misc.toolkit import interpolate\n",
    "from cflearn.misc.toolkit import toggle_optimizer\n",
    "from cflearn.modules.blocks import Lambda\n",
    "from cflearn.modules.blocks import UpsampleConv2d\n",
    "\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self):  # type: ignore\n",
    "        super().__init__()\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        self.register_buffer(\"real_label\", torch.tensor(1.0))\n",
    "        self.register_buffer(\"fake_label\", torch.tensor(0.0))\n",
    "\n",
    "    def expand_target(self, tensor: Tensor, target_is_real: bool) -> Tensor:\n",
    "        target = self.real_label if target_is_real else self.fake_label\n",
    "        return target.expand_as(tensor)  # type: ignore\n",
    "\n",
    "    def forward(self, predictions: Tensor, target_is_real: bool) -> Tensor:\n",
    "        target_tensor = self.expand_target(predictions, target_is_real)\n",
    "        loss = self.loss(predictions, target_tensor)\n",
    "        return loss\n",
    "\n",
    "def make_generator(in_channels, img_size, latent_channels):\n",
    "    return nn.Sequential(\n",
    "        Lambda(lambda t: t.view(-1, latent_channels, 4, 4), name=\"reshape\"),\n",
    "        nn.Conv2d(latent_channels, 128, 1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(128),\n",
    "        UpsampleConv2d(128, 64, kernel_size=3, padding=1, factor=2),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(64),\n",
    "        UpsampleConv2d(64, 32, kernel_size=3, padding=1, factor=2),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(32),\n",
    "        UpsampleConv2d(32, in_channels, kernel_size=3, padding=1, factor=2),\n",
    "        Lambda(lambda t: interpolate(t, size=img_size, mode=\"bilinear\")),\n",
    "        nn.Tanh(),\n",
    "    )\n",
    "\n",
    "def make_discriminator(in_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, 16, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(16),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(16, 32, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(32, 64, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(64, 128, 3, padding=1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Complex Models\n",
    "\n",
    "We'll first show you how to implement a simple `GAN` model, which can illustrate a minimum subset of the provided functionalities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cflearn.register_custom_module(\"my_gan0\", allow_duplicate=False)\n",
    "class MyGAN0(cflearn.CustomModule):\n",
    "    def __init__(self, in_channels, img_size, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        latent_channels = latent_dim // 16\n",
    "        self.generator = make_generator(in_channels, img_size, latent_channels)\n",
    "        self.discriminator = make_discriminator(in_channels)\n",
    "        self.loss = GANLoss()\n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        z = torch.randn(num_samples, self.latent_dim, device=self.device)\n",
    "        return self.generator(z)\n",
    "\n",
    "    def forward(self, net):\n",
    "        self.sample(len(net))\n",
    "    \n",
    "    @property\n",
    "    def g_parameters(self):\n",
    "        return list(self.generator.parameters())\n",
    "\n",
    "    @property\n",
    "    def d_parameters(self):\n",
    "        return list(self.discriminator.parameters())\n",
    "\n",
    "    def train_step(self, batch, optimizers) -> StepOutputs:\n",
    "        net = batch[INPUT_KEY]\n",
    "        opt_g = optimizers[\"core.g_parameters\"]\n",
    "        opt_d = optimizers[\"core.d_parameters\"]\n",
    "        # generator step\n",
    "        # `toggle_optimizer` can help you focus on `opt_g`'s gradients, and ignore other gradients\n",
    "        with toggle_optimizer(self, opt_g):\n",
    "            opt_g.zero_grad()\n",
    "            sampled = self.sample(len(net))\n",
    "            pred_fake = self.discriminator(sampled)\n",
    "            g_loss = self.loss(pred_fake, target_is_real=True)\n",
    "            g_loss.backward()\n",
    "            opt_g.step()\n",
    "        # discriminator step\n",
    "        # `toggle_optimizer` can help you focus on `opt_d`'s gradients, and ignore other gradients\n",
    "        with toggle_optimizer(self, opt_d):\n",
    "            opt_d.zero_grad()\n",
    "            pred_real = self.discriminator(net)\n",
    "            loss_d_real = self.loss(pred_real, target_is_real=True)\n",
    "            pred_fake = self.discriminator(sampled.detach())\n",
    "            loss_d_fake = self.loss(pred_fake, target_is_real=False)\n",
    "            d_loss = 0.5 * (loss_d_fake + loss_d_real)\n",
    "            d_loss.backward()\n",
    "            opt_d.step()\n",
    "        # finalize\n",
    "        forward_results = {PREDICTIONS_KEY: sampled}\n",
    "        loss_dict = {\n",
    "            \"g\": g_loss.item(),\n",
    "            \"d\": d_loss.item(),\n",
    "            \"d_fake\": loss_d_fake.item(),\n",
    "            \"d_real\": loss_d_real.item(),\n",
    "        }\n",
    "        return StepOutputs(forward_results, loss_dict)\n",
    "\n",
    "    def evaluate_step(self, loader) -> MetricsOutputs:\n",
    "        loss_items = {}\n",
    "        for i, batch in enumerate(loader):\n",
    "            # `to_device` can help you put the tensors in a `dict` to a specific device\n",
    "            batch = to_device(batch, self.device)\n",
    "            net = batch[INPUT_KEY]\n",
    "            sampled = self.sample(len(net))\n",
    "            pred_fake = self.discriminator(sampled)\n",
    "            g_loss = self.loss(pred_fake, target_is_real=True)\n",
    "            pred_real = self.discriminator(net)\n",
    "            d_loss = self.loss(pred_real, target_is_real=True)\n",
    "            loss_items.setdefault(\"g\", []).append(g_loss.item())\n",
    "            loss_items.setdefault(\"d\", []).append(d_loss.item())\n",
    "        # gather\n",
    "        mean_loss_items = {k: sum(v) / len(v) for k, v in loss_items.items()}\n",
    "        loss = sum(mean_loss_items.values())\n",
    "        mean_loss_items[cflearn.LOSS_KEY] = loss\n",
    "        return MetricsOutputs(-loss, mean_loss_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `GAN` model covers most of the important concepts that you need to know to build your own 'complex' models. Let's dive into them step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__init__` / `forward`\n",
    "\n",
    "They are the easiest parts, because their behaviours are the same as the behaviours mentioned in the [Other Models](#Other-Models) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `g_parameters` / `d_parameters`\n",
    "\n",
    "When using 'complex' models, we often need to use more than one optimizer (otherwise the 'one-stage' models should already satisfy the needs). In this case, we need to specify what parameters should each optimizer focuses on. In `carefree-learn`, we can do so by simply defining each group of parameters under a `property` and setup the `optimizer_settings` config in the `fit_*` APIs (e.g. `fit_ml` & `fit_cv`).\n",
    "\n",
    "For the example above, we defined a `g_parameters` property and a `d_parameters` property, so the `optimizer_settings` should be something like:\n",
    "\n",
    "```python\n",
    "cflearn.api.fit_cv(\n",
    "    ...,\n",
    "    optimizer_settings = dict(\n",
    "        core.g_parameters=dict(\n",
    "            optimizer=\"adam\",\n",
    "            scheduler=\"warmup\",\n",
    "        ),\n",
    "        core.d_parameters=dict(\n",
    "            optimizer=\"adam\",\n",
    "            scheduler=\"warmup\",\n",
    "        ),\n",
    "    ),\n",
    "    ...,\n",
    ")\n",
    "```\n",
    "\n",
    "You might notice that the keys of `optimizer_settings` is prefixed with `core.`, this is because `carefree-learn` again 'wrapped' your models in order to integrate them into its pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `train_step`\n",
    "\n",
    "Now comes to the hard part: the training step. There are many complex functionalities lie behind this method, but for this simple example we only used two of them: `batch` & `optimizers`. These two arguments are likely to be the minimum requirements of your `train_step`, because in general you should at least:\n",
    "1. Do some forward calculations with your model and the input data (the `batch`).\n",
    "2. Do some backward updates with the `optimizers`.\n",
    "\n",
    "Let's break the `train_step` implementation above to see what is going on line by line:\n",
    "\n",
    "```python\n",
    "class StepOutputs(NamedTuple):\n",
    "    forward_results: Dict[str, Tensor]\n",
    "    loss_dict: Dict[str, float]\n",
    "\n",
    "class MyGAN0:\n",
    "    ...\n",
    "    def train_step(self, batch, optimizers) -> StepOutputs:\n",
    "        net = batch[INPUT_KEY]\n",
    "        opt_g = optimizers[\"core.g_parameters\"]\n",
    "        opt_d = optimizers[\"core.d_parameters\"]\n",
    "        ...\n",
    "```\n",
    "\n",
    "The first line:\n",
    "\n",
    "```python\n",
    "def train_step(self, batch, optimizers) -> StepOutputs:\n",
    "```\n",
    "\n",
    "Indicates that:\n",
    "1. We 'requires' `batch` & `optimizers` for our `train_step`.\n",
    "2. We need to return a `StepOutputs` at the end of this `train_step`.\n",
    "\n",
    "> For some later sections, you will find that we will 'require' more and more stuffs for our `train_step`. It is at your wish to 'require' what you need from the [Full set of Functionalities](#Full-set-of-Functionalities)!\n",
    ">\n",
    "> Unlike some other methods, the order of the arguments in `train_step` is not important, but the naming is. So you can actually write\n",
    "\n",
    "```python\n",
    "def train_step(self, optimizers, batch) -> StepOutputs:\n",
    "    ...\n",
    "```\n",
    "\n",
    "The second line:\n",
    "\n",
    "```python\n",
    "net = batch[INPUT_KEY]\n",
    "```\n",
    "\n",
    "extracted the input tensor from the `batch`.\n",
    "\n",
    "The third & fourth line:\n",
    "\n",
    "```python\n",
    "opt_g = optimizers[\"core.g_parameters\"]\n",
    "opt_d = optimizers[\"core.d_parameters\"]\n",
    "```\n",
    "\n",
    "extracted the corresponding optimizer of each group of parameters. You might notice that the key of `optimizers` matches the key of `optimizer_settings`, which is pretty reasonable.\n",
    "\n",
    "The rest of `train_step`'s implementations are basically some typical `GAN` implementations, but the `finalize` part still needs to pay some attention to:\n",
    "```python\n",
    "# finalize\n",
    "forward_results = {PREDICTIONS_KEY: sampled}\n",
    "loss_dict = {\n",
    "    \"g\": g_loss.item(),\n",
    "    \"d\": d_loss.item(),\n",
    "    \"d_fake\": loss_d_fake.item(),\n",
    "    \"d_real\": loss_d_real.item(),\n",
    "}\n",
    "return StepOutputs(forward_results, loss_dict)\n",
    "```\n",
    "\n",
    "There are two `dict` constructed here: the `forward_results` and the `loss_dict`. The `forward_results` is not utilized in `carefree-learn` yet, and the `loss_dict` is rarely used as well, except:\n",
    "1. validation dataset is not provided, so `carefree-learn` will use this `loss_dict` to calculate metrics.\n",
    "2. `mlflow` callback is enabled, so `carefree-learn` will record these losses.\n",
    "\n",
    "> So for simplicity, it's completely OK to return two empty `dict` here:\n",
    "\n",
    "```python\n",
    "# finalize\n",
    "return StepOutputs({}, {})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `evaluate_step`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full set of Functionalities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Encodings\n",
    "\n",
    "What makes Machine Learning tasks different from other tasks is that some of the data-preprocessing methods are 'trainable' (e.g. embedding). In this case, we need to integrate these methods into our models rather than simply put them in a separate place.\n",
    "\n",
    "It is OK to implement these methods in our custom models every time when we need them, but that will cause a **LOT** of boilerplate codes. `carefree-learn` therefore extracted them into an `Encoder` module, and exposed its settings in the APIs for you to utilize it easily.\n",
    "\n",
    "The definitions related to the `Encoder` are pretty simple:\n",
    "\n",
    "```python\n",
    "class EncodingSettings(NamedTuple):\n",
    "    dim: int\n",
    "    methods: Union[str, List[str]] = \"embedding\"\n",
    "    method_configs: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        settings: Dict[int, EncodingSettings],\n",
    "        *,\n",
    "        # there are a few more kwargs here, which will be covered in the `Optimizations` section\n",
    "        ...\n",
    "    ):\n",
    "        ...\n",
    "```\n",
    "\n",
    "The `settings` of the `Encoder` is what we should mainly pay attention to: it's a mapping that maps the column index to its corresponding `EncodingSettings`.\n",
    "\n",
    "For example, if:\n",
    "- Our input features, `x`, has 10 columns, then the column index should be [0, 1, 2, ..., 9]\n",
    "- The `0`th, `5`th & `7`th column are categorical columns, and they have `5`, `7` & `11` unique values respectively.\n",
    "- We want to apply `one_hot` to the `0`th & `5`th column.\n",
    "- We want to apply `embedding` to the `5`th & `7`th column.\n",
    "\n",
    "Then the corresponding setup should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embeddings): ModuleDict(\n",
       "    (-1): Embedding(\n",
       "      (core): Lambda(embedding: 12 -> 4)\n",
       "    )\n",
       "  )\n",
       "  (one_hot_encoders): ModuleDict(\n",
       "    (5): OneHot(\n",
       "      (core): Lambda(one_hot_7)\n",
       "    )\n",
       "    (7): OneHot(\n",
       "      (core): Lambda(one_hot_11)\n",
       "    )\n",
       "  )\n",
       "  (embedding_dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cflearn.models.ml.encoders import Encoder\n",
    "from cflearn.models.ml.encoders import EncodingSettings\n",
    "\n",
    "Encoder(\n",
    "    {\n",
    "        0: EncodingSettings(5),\n",
    "        5: EncodingSettings(7, [\"one_hot\", \"embedding\"]),\n",
    "        7: EncodingSettings(11, \"one_hot\"),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that the `one_hot_encoders` part is pretty straight forward: we initialized a `ModuleDict` which mapped the column index into its corresponding `OneHot` encoder, and the dimension matches exactly to the number of unique values. However, the `embeddings` part is a little weird: we only initialized one `Embedding` module, and the dimension is `12`, which is exactly `5+7` - the sum of the number of unique values.\n",
    "\n",
    "This is due to a special mechanism in `carefree-learn` - the `fast_embedding` mechanism. We will cover the details in the [next section](#Optimizations), for now let's just see how to disable this mechanism and make our `Encoder` looks 'normal':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embeddings): ModuleDict(\n",
       "    (0): Embedding(\n",
       "      (core): Lambda(embedding: 5 -> 4)\n",
       "    )\n",
       "    (5): Embedding(\n",
       "      (core): Lambda(embedding: 7 -> 4)\n",
       "    )\n",
       "  )\n",
       "  (one_hot_encoders): ModuleDict(\n",
       "    (5): OneHot(\n",
       "      (core): Lambda(one_hot_7)\n",
       "    )\n",
       "    (7): OneHot(\n",
       "      (core): Lambda(one_hot_11)\n",
       "    )\n",
       "  )\n",
       "  (embedding_dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Encoder(\n",
    "    {\n",
    "        0: EncodingSettings(5),\n",
    "        5: EncodingSettings(7, [\"one_hot\", \"embedding\"]),\n",
    "        7: EncodingSettings(11, \"one_hot\"),\n",
    "    },\n",
    "    config={\n",
    "        \"use_fast_embedding\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now the `embedding` part looks exactly the same as the `one_hot_encoders` part: we initialized a `ModuleDict` which mapped the column index into its corresponding `Embedding` encoder, and the dimension matches exactly to the number of unique values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizations\n",
    "\n",
    "`carefree-learn` not only provides *carefree* APIs for easier usages, but also did quite a few optimizations to make training on tabular datasets faster than other similar libraries. In this section we'll introduce some techniques `carefree-learn` adopted under the hood, and will show how much performance boost we've obtained with them.\n",
    "\n",
    "#### One Hot Encoding\n",
    "\n",
    "A `one_hot` encoding basically encodes categorical features as a one-hot numeric array, as defined in [`sklearn`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html). Suppose we have 5 classes in total, then:\n",
    "\n",
    "$$\n",
    "\\text{OneHot}(0) = [1,0,0,0,0] \\\\\n",
    "\\text{OneHot}(3) = [0,0,0,1,0]\n",
    "$$\n",
    "\n",
    "We can figure out that this kind of encoding is **static**, which means it will not change during the training process. In this case, we can cache down all the encodings and access them through indexing. This will speed up the encoding process for ~60x:\n",
    "\n",
    "> You need to install `scikit-learn` to run the following examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3 µs ± 915 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "226 µs ± 28.2 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "num_data = 10000\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "\n",
    "x = np.random.randint(0, num_classes, num_data).reshape([-1, 1])\n",
    "enc = OneHotEncoder(sparse=False).fit(x)\n",
    "x_one_hot = enc.transform(x)\n",
    "\n",
    "target_indices = np.random.permutation(num_data)[:batch_size]\n",
    "x_target = x[target_indices]\n",
    "\n",
    "assert np.allclose(x_one_hot[target_indices], enc.transform(x_target))\n",
    "%timeit x_one_hot[target_indices]\n",
    "%timeit enc.transform(x_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Although caching can boost performance, it is at the cost of consuming much more memories. A better solution should be caching sparse tensors instead of dense ones, but `PyTorch` has not supported sparsity good enough. See [Sparsity](#Sparsity) section for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding\n",
    "\n",
    "An `embedding` encoding actually borrows from **N**atual **L**anguage **P**rocessing (**NLP**) where they converted (sparse) input words into dense embeddings with embedding look up. It is quite trivial to turn categorical features into embeddings with the same look up techniques, but tabular datasets hold a different property compared with **NLP**: tabular datasets will maintain many embedding tables because they have different categorical features with different number of values, while in **NLP** it only need to maintain one embedding table in most cases.\n",
    "\n",
    "Since `embedding` is a **dynamic** encoding which contains trainable parameters, we cannot cache them beforehand like we did to `one_hot`. However, we can still optimize it with *fast embedding*. A *fast embedding* basically unifies the embedding dimension of different categorical features, so one unified embedding table is sufficient for the whole `embedding` process.\n",
    "\n",
    "There's one more thing we need to take care of when applying *fast embedding*: we need to *increment* the values of each categorical features. Here's a minimal example to illustrate this. Suppose we have two categorical features ($x_1, x_2$) with 2 and 3 classes respectively, then our embedding table will contain 5 rows:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\text{---} \\text{---} \\ v_1 \\ \\text{---} \\text{---} \\\\\n",
    "    \\text{---} \\text{---} \\ v_2 \\ \\text{---} \\text{---} \\\\\n",
    "    \\text{---} \\text{---} \\ v_3 \\ \\text{---} \\text{---} \\\\\n",
    "    \\text{---} \\text{---} \\ v_4 \\ \\text{---} \\text{---} \\\\\n",
    "    \\text{---} \\text{---} \\ v_5 \\ \\text{---} \\text{---}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In this table, the first two rows belong to $x_1$, while the last three rows belong to $x_2$. However, as we defined above, $x_1\\in\\{0,1\\}$ and $x_2\\in\\{0,1,2\\}$. In order to assign $v_3,v_4,v_5$ to $x_2$, we need to *increment* $x_2$ by $2$ (which is the number of choices $x_1$ could have). After *increment*, we have $x_2\\in\\{2,3,4\\}$ so it can successfully look up $v_3,v_4,v_5$.\n",
    "\n",
    "Note that the *incremented* indices are **static**, so `carefree-learn` will cache these indices to avoid duplicate calculations when *fast embedding* is applied.\n",
    "\n",
    "Since the embedding dimensions are unified, *fast embedding* actually reduces the flexibility a little bit, but it can speed up the encoding process for ~16x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.1 µs ± 1.8 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "474 µs ± 10.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn import Embedding\n",
    "from cflearn.misc.toolkit import to_torch\n",
    "\n",
    "dim = 20\n",
    "batch_size = 256\n",
    "\n",
    "features = []\n",
    "embeddings = []\n",
    "for i in range(dim):\n",
    "    # 5, 10, 15, 20\n",
    "    num_classes = math.ceil((i + 1) / 5) * 5\n",
    "    x = np.random.randint(0, num_classes, batch_size).reshape([-1, 1])\n",
    "    embedding = Embedding(num_classes, 1)\n",
    "    embeddings.append(embedding)\n",
    "    features.append(x)\n",
    "\n",
    "fast_embedding = Embedding(250, 1)\n",
    "tensor = to_torch(np.hstack(features)).to(torch.long)\n",
    "\n",
    "def f1():\n",
    "    return fast_embedding(tensor)\n",
    "\n",
    "def f2():\n",
    "    embedded = []\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        embedded.append(embedding(tensor[..., i:i+1]))\n",
    "    return torch.cat(embedded, dim=1)\n",
    "\n",
    "assert f1().shape == f2().shape\n",
    "%timeit f1()\n",
    "%timeit f2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Theoratically, `embedding` encoding is nothing more than a `one_hot` encoding followed by a linear projection, so it should be fast enough if we apply sparse matrix multiplications between `one_hot` encodings and a block diagnal `embedding` look up table. However as mentioned in [One Hot Encoding](#One-Hot-Encoding) section, `PyTorch` has not supported sparsity good enough. See [Sparsity](#Sparsity) section for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparsity\n",
    "\n",
    "It is quite trivial that the `one_hot` encoding actually outputs a sparse matrix with sparsity equals to:\n",
    "\n",
    "$$\n",
    "1-\\frac{1}{\\text{num_classes}}\n",
    "$$\n",
    "\n",
    "So the sparsity will exceed 90% when `num_classes` is greater than 10, therefore it is quite natural to think of leveraging sparse data structures to cache these `one_hot` encodings. What's better is that the `embedding` encoding could be represented as sparse matrix multiplications between `one_hot` encodings and a block diagnal `embedding` look up table, so **THEORATICALLY** (🤣) we could reuse the `one_hot` encodings to get the `embedding` encodings efficiently.\n",
    "\n",
    "Unfortunately, although [`scipy`](https://docs.scipy.org/doc/scipy/reference/sparse.html) supports sparse matrices pretty well, `pytorch` has not yet supported them good enough. So we'll stick to the dense solutions mentioned above, but will switch to the sparse ones iff `pytorch` releases some fancy sparsity supports!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
