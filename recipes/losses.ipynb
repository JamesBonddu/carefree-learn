{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Losses Recipes\n",
    "\n",
    "In this page, we will show you how to customize your own losses. In `carefree-learn`, it is fairly easy to define various kinds of losses (ML, CV, etc.) with a unified API `register_loss_module`.\n",
    "\n",
    "> You might notice that if you run the blocks with `register_loss_module` calls for more than once, `carefree-learn` will throw a warning which says \" '...' has already been registered \", and your changes will have no effect. This is intentional because normally we **DO NOT** want to register anything for more than once.\n",
    "> \n",
    "> However, if you are using some interactive developing tools (e.g. Jupyter Notebook), it is very common to modify the implementations for more than once. In this case, we can set `allow_duplicate=True` in the `register_loss_module` functions to bypass this check. And of course, this should **NEVER** happen in production for safety!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "\n",
    "- [Typical Losses](#Typical-Losses)\n",
    "- [Complex Losses](#Complex-Losses)\n",
    "- [Integration](#Integration)\n",
    "  - [Single Loss](#Single-Loss)\n",
    "  - [Multi-Task Loss](#Multi-Task-Loss)\n",
    "- [Q&A](#Q&A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You might also notice that the class name defined below somehow matches the registered name. This is not required, since `carefree-learn` only cares about the name that you pass to the `register_loss_module` function, and will not check the actual class name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import cflearn\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Dict\n",
    "from cflearn.constants import LOSS_KEY\n",
    "from cflearn.constants import LABEL_KEY\n",
    "from cflearn.constants import PREDICTIONS_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typical Losses\n",
    "\n",
    "In the most typical cases, a loss function should receive a `predictions` and a `labels` to calculate the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typical classification loss - cross entropy\n",
    "@cflearn.register_loss_module(\"my_cross_entropy\", allow_duplicate=False)\n",
    "class MyCrossEntropy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    # logits : [N, K, ...]\n",
    "    # labels : [N, 1, ...]\n",
    "    def forward(self, predictions: Tensor, labels: Tensor) -> Tensor:\n",
    "        return self.ce(predictions, labels.squeeze(1))\n",
    "\n",
    "# typical regression loss - mean absolute error\n",
    "@cflearn.register_loss_module(\"my_mae\", allow_duplicate=False)\n",
    "class MyMAE(nn.Module):\n",
    "    # logits : [N, ...]\n",
    "    # labels : [N, ...]\n",
    "    def forward(self, predictions: Tensor, labels: Tensor) -> Tensor:\n",
    "        return (predictions - labels).abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You might notice that the returned `Tensor` is not 'reduced', that's because `carefree-learn` supports specifying the `reduction` (default: `mean`) of your losses (as `pytorch` does). See the following `Usages` section for more details.\n",
    ">\n",
    "> However, you can still 'reduce' your loss in the definition. In this case, your loss will return the same values no matter what the `reduction` is ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "0.8284955024719238\n",
      "torch.Size([100])\n",
      "3.1444976329803467\n",
      "314.44976806640625\n",
      "\n",
      "torch.Size([10, 10, 10])\n",
      "0.7982355952262878\n",
      "torch.Size([10, 10, 10])\n",
      "3.1760029792785645\n",
      "3176.0029296875\n",
      "\n",
      "torch.Size([100, 5])\n",
      "\n",
      "torch.Size([4, 5, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.randn(100, 5)\n",
    "labels = torch.argmax(logits, dim=1, keepdim=True)\n",
    "inv_labels = torch.argmin(logits, dim=1, keepdim=True)\n",
    "fw = {PREDICTIONS_KEY: logits}\n",
    "b = {LABEL_KEY: labels}\n",
    "ib = {LABEL_KEY: inv_labels}\n",
    "my_ce = cflearn.api.make_loss(\"my_cross_entropy\")\n",
    "my_ce_sum = cflearn.api.make_loss(\"my_cross_entropy\", reduction=\"sum\")\n",
    "print(my_ce.core(logits, labels).shape)\n",
    "print(my_ce(fw, b)[LOSS_KEY].item())\n",
    "print(my_ce.core(logits, inv_labels).shape)\n",
    "print(my_ce(fw, ib)[LOSS_KEY].item())\n",
    "print(my_ce_sum(fw, ib)[LOSS_KEY].item())\n",
    "print()\n",
    "logits = torch.randn(10, 5, 10, 10)\n",
    "labels = torch.argmax(logits, dim=1, keepdim=True)\n",
    "inv_labels = torch.argmin(logits, dim=1, keepdim=True)\n",
    "inv_labels = torch.argmin(logits, dim=1, keepdim=True)\n",
    "fw = {PREDICTIONS_KEY: logits}\n",
    "b = {LABEL_KEY: labels}\n",
    "ib = {LABEL_KEY: inv_labels}\n",
    "print(my_ce.core(logits, labels).shape)\n",
    "print(my_ce(fw, b)[LOSS_KEY].item())\n",
    "print(my_ce.core(logits, inv_labels).shape)\n",
    "print(my_ce(fw, ib)[LOSS_KEY].item())\n",
    "print(my_ce_sum(fw, ib)[LOSS_KEY].item())\n",
    "\n",
    "predictions = torch.randn(100, 5)\n",
    "labels = predictions - 0.123\n",
    "my_mae = cflearn.api.make_loss(\"my_mae\")\n",
    "print()\n",
    "print(my_mae.core(predictions, labels).shape)\n",
    "predictions = torch.randn(4, 5, 64, 64)\n",
    "labels = predictions - 0.123\n",
    "print()\n",
    "print(my_mae.core(predictions, labels).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Losses\n",
    "\n",
    "In some complex situations, we may:\n",
    "- have multiple values in our predictions / inputs (e.g. multi-style transfer).\n",
    "- need to record multiple losses for debug / verbose purpose.\n",
    "\n",
    "`carefree-learn` therefore supports your custom losses to:\n",
    "- receive a `dict` of `Tensor`s.\n",
    "- return a `dict` of `Tensor`s, in which the value of `LOSS_KEY` should indicate the final loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "@cflearn.register_loss_module(\"my_vq_vae_loss\", allow_duplicate=False)\n",
    "class MyVQVAELoss(nn.Module):\n",
    "    def forward(\n",
    "        self,\n",
    "        forward_results: Dict[str, Tensor],\n",
    "        batch: Dict[str, Tensor],\n",
    "    ) -> Dict[str, Tensor]:\n",
    "        # reconstruction loss\n",
    "        target = batch[LABEL_KEY]\n",
    "        reconstruction = forward_results[PREDICTIONS_KEY]\n",
    "        mse = F.mse_loss(reconstruction, target)\n",
    "        # vq & commit loss\n",
    "        z_e = forward_results[\"z_e\"]\n",
    "        z_q_g = forward_results[\"z_q_g\"]\n",
    "        vq_loss = F.mse_loss(z_q_g, z_e.detach())\n",
    "        commit_loss = F.mse_loss(z_e, z_q_g.detach())\n",
    "        # gather\n",
    "        loss = mse + vq_loss + commit_loss\n",
    "        return {\"mse\": mse, \"commit\": commit_loss, LOSS_KEY: loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naming is important here - you should use `forward_results` & `batch` to let `carefree-learn` knows that you require the full data instead of one single `Tensor`.\n",
    "\n",
    "You can also simplify your implementation with this design if you only require parts of the full data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cflearn.register_loss_module(\"my_vq_vae_loss2\", allow_duplicate=False)\n",
    "class MyVQVAELoss2(nn.Module):\n",
    "    def forward(\n",
    "        self,\n",
    "        forward_results: Dict[str, Tensor],\n",
    "        target: Tensor,\n",
    "    ) -> Dict[str, Tensor]:\n",
    "        # reconstruction loss\n",
    "        reconstruction = forward_results[PREDICTIONS_KEY]\n",
    "        mse = F.mse_loss(reconstruction, target)\n",
    "        # vq & commit loss\n",
    "        z_e = forward_results[\"z_e\"]\n",
    "        z_q_g = forward_results[\"z_q_g\"]\n",
    "        vq_loss = F.mse_loss(z_q_g, z_e.detach())\n",
    "        commit_loss = F.mse_loss(z_e, z_q_g.detach())\n",
    "        # gather\n",
    "        loss = mse + vq_loss + commit_loss\n",
    "        return {\"mse\": mse, \"commit\": commit_loss, LOSS_KEY: loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some rare scenarios, we may even need the training `state` (e.g. current `step` / `epoch`) to calculate our losses. This is also accessible in `carefree-learn` by simply add a `state` argument to the `forward` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from cflearn.protocol import TrainerState\n",
    "\n",
    "@cflearn.register_loss_module(\"my_state_loss\", allow_duplicate=False)\n",
    "class MyStateLoss(nn.Module):\n",
    "    def forward(\n",
    "        self,\n",
    "        forward_results: Dict[str, Tensor],\n",
    "        target: Tensor,\n",
    "        state: Optional[TrainerState] = None,\n",
    "    ) -> Dict[str, Tensor]:\n",
    "        # to something\n",
    "        return torch.tensor([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that the `state` is not always available (for example, at testing stage), so we always need to handle `if state is None` condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mse': tensor(0.0151), 'commit': tensor(0.0548), 'loss': tensor(0.1246)}\n",
      "{'mse': tensor(0.0151), 'commit': tensor(0.0548), 'loss': tensor(0.1246)}\n"
     ]
    }
   ],
   "source": [
    "target = torch.randn(4, 3, 32, 32)\n",
    "reconstruction = target + 0.123\n",
    "code = torch.randn(4, 32)\n",
    "forward_results = {\n",
    "    PREDICTIONS_KEY: reconstruction,\n",
    "    \"z_e\": code,\n",
    "    \"z_q_g\": code + 0.234,\n",
    "}\n",
    "batch = {\n",
    "    LABEL_KEY: target,\n",
    "}\n",
    "my_vq_vae_loss = cflearn.api.make_loss(\"my_vq_vae_loss\")\n",
    "my_vq_vae_loss2 = cflearn.api.make_loss(\"my_vq_vae_loss2\")\n",
    "print(my_vq_vae_loss(forward_results, batch))\n",
    "print(my_vq_vae_loss2(forward_results, batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration\n",
    "\n",
    "After defining our own losses, we need to know how to integrate them in existing `carefree-learn` pipelines for training, testing and deploying. Basically, losses could be specified across various APIs with `loss_name` and `loss_config`. We will use `fit_ml` to demonstrate the core concepts, and the same recipes could be applied elsewhere.\n",
    "\n",
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.random([100, 5])\n",
    "y = np.random.random([100, 1])\n",
    "common_kwargs = dict(\n",
    "    x_train=x,\n",
    "    y_train=y,\n",
    "    x_valid=x,\n",
    "    y_valid=y,\n",
    "    core_name=\"linear\",\n",
    "    input_dim=5,\n",
    "    output_dim=1,\n",
    "    is_classification=False,\n",
    "    fixed_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Loss\n",
    "\n",
    "In this case, simply specify the `loss_name` to our target loss, and the `loss_config` should be the `kwargs` that will be passed to your loss's `__init__` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>>> MyFooLoss.foo: 1.2345\n",
      "\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    Linear                                   [-1, 5]                                  [-1, 1]                    6\n",
      "      Linear                                 [-1, 5]                                  [-1, 1]                    6\n",
      "========================================================================================================================\n",
      "Total params: 6\n",
      "Trainable params: 6\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-23_16-14-20-410837\\checkpoints\n",
      "| epoch  -1  [-1 / 1] [0.046s] | loss : 1.212904 | score : -1.21290 |\n"
     ]
    }
   ],
   "source": [
    "@cflearn.register_loss_module(\"my_foo_loss\", allow_duplicate=False)\n",
    "class MyFooLoss(nn.Module):\n",
    "    def __init__(self, foo):\n",
    "        super().__init__()\n",
    "        self.foo = foo\n",
    "        print(f\"\\n>>>>>> MyFooLoss.foo: {foo}\\n\")\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        return self.foo * (logits - labels).abs()\n",
    "\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    loss_name=\"my_foo_loss\",\n",
    "    loss_config=dict(foo=1.2345),\n",
    "    **common_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Task Loss\n",
    "\n",
    "In some cases, we might want to use multiple losses at the same time. Although it is often recommended to define a new loss to achieve this, `carefree-learn` still provides a `multi_task` 'hook' for accessibility. The format of `loss_name` in this case should be:\n",
    "\n",
    "```text\n",
    "multi_task:{loss1},{loss2},...,{lossk}\n",
    "```\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    Linear                                   [-1, 5]                                  [-1, 1]                    6\n",
      "      Linear                                 [-1, 5]                                  [-1, 1]                    6\n",
      "========================================================================================================================\n",
      "Total params: 6\n",
      "Trainable params: 6\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-07-23_16-14-20-938835\\checkpoints\n",
      "| epoch  -1  [-1 / 1] [0.014s] | loss : 0.384980 | mae : 0.276903 | mse : 0.108077 | score : -0.38498 |\n"
     ]
    }
   ],
   "source": [
    "m = cflearn.api.fit_ml(\n",
    "    loss_name=\"multi_task:mae,mse\",\n",
    "    **common_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When using the `multi_task` hook, all losses will be sumed up to construct the final loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A\n",
    "\n",
    "## Does the order of the `forward` arguments matter?\n",
    "\n",
    "- Yes, it matters, we should always use `predictions` as the first argument, and `inputs` as the second. This is aligned to the `pytorch`'s API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
