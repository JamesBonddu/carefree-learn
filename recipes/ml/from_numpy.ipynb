{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ML Recipes - From `numpy`\n",
    "\n",
    "In this page, we will show you how to solve your own ML tasks on top of a `numpy` dataset.\n",
    "\n",
    "> It is recommended to go through some basic recipes ([`models`](../models.ipynb), [`losses`](../losses.ipynb), [`metrics`](../metrics.ipynb)) first, but if you just want to utilize `carefree-learn` as soon as possible, this page is pretty self-contained as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "\n",
    "- [No Data Processing](#No-Data-Processing)\n",
    "  - [Use Existing Modules](#Use-Existing-Modules)\n",
    "  - [Customize Modules](#Customize-Modules)\n",
    "    - [Customize Models](#Customize-Models)\n",
    "    - [Customize Losses](#Customize-Losses)\n",
    "    - [Customize Metrics](#Customize-Metrics)\n",
    "- [With Data Processing](#With-Data-Processing)\n",
    "  - [Out of the Loop](#Out-of-the-Loop)\n",
    "  - [Utilize the Integrated `IMLData` System](#Utilize-the-Integrated-IMLData-System)\n",
    "    - [Example](#Example)\n",
    "    - [Explanations](#Explanations)\n",
    "      - [General](#General)\n",
    "      - [`build_with`](#build_with)\n",
    "      - [`preprocess`](#preprocess)\n",
    "      - [`dumps` / `loads`](#dumps-/-loads)\n",
    "- [Processing Labels](#Processing-Labels)\n",
    "- [Optional Callbacks](#Optional-Callbacks)\n",
    "  - [`get_num_samples`](#get_num_samples)\n",
    "  - [`fetch_batch`](#fetch_batch)\n",
    "  - [`postprocess_batch`](#postprocess_batch)\n",
    "  - [`postprocess_results`](#postprocess_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e5797cf4d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import cflearn\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Dict\n",
    "\n",
    "np.random.seed(142857)\n",
    "torch.manual_seed(142857)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Data Processing\n",
    "\n",
    "We will first jump into the typical situation where our data is already 'nice and clean', so we can use it as-is without any data processing procedure.\n",
    "\n",
    "Here's the toy dataset that we'll use through out this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 10), (1000, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 10\n",
    "num_sample = 1000\n",
    "\n",
    "x = np.random.random([num_sample, dim])\n",
    "w = np.random.random([dim, 1])\n",
    "y = x.dot(w)\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Existing Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`carefree-learn` already provides a bunch of modules that can be used directly in your ML tasks. We will show you how to specify each of them in this section, and illustrate how to customize each of them in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "                                Internal Default Configurations Used by `carefree-learn`                                \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "                             train_samples   |   1000\n",
      "                             valid_samples   |   None\n",
      "                         max_snapshot_file   |   25\n",
      "                                 input_dim   |   10\n",
      "                                 workplace   |   _logs\\2022-08-18_19-45-53-344718\n",
      "                             monitor_names   |   ['mean_std', 'plateau']\n",
      "                      additional_callbacks   |   ['_log_metrics_msg', '_inject_loader_name']\n",
      "                   log_metrics_msg_verbose   |   True\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    FCNN                                    [-1, 10]                                  [-1, 1]                1,441\n",
      "      Sequential                            [-1, 10]                                  [-1, 1]                1,441\n",
      "        Mapping-0                           [-1, 10]                                 [-1, 32]                  352\n",
      "          Linear                            [-1, 10]                                 [-1, 32]                  352\n",
      "          ReLU                              [-1, 32]                                 [-1, 32]                    0\n",
      "        Mapping-1                           [-1, 32]                                 [-1, 32]                1,056\n",
      "          Linear                            [-1, 32]                                 [-1, 32]                1,056\n",
      "          ReLU                              [-1, 32]                                 [-1, 32]                    0\n",
      "        Linear                              [-1, 32]                                  [-1, 1]                   33\n",
      "========================================================================================================================\n",
      "Total params: 1,441\n",
      "Trainable params: 1,441\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.01\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "| epoch  44  [8 / 8] [1.367s] | mae : 0.006016 | mse : 0.000073 | score : -0.00304 |\n",
      "| epoch  88  [8 / 8] [1.197s] | mae : 0.001257 | mse : 0.000016 | score : -0.00063 |\n",
      "| epoch 132  [8 / 8] [1.641s] | mae : 0.000774 | mse : 0.000015 | score : -0.00039 |\n",
      "| epoch 176  [8 / 8] [1.235s] | mae : 0.000748 | mse : 0.000015 | score : -0.00038 |\n",
      "| epoch 220  [8 / 8] [1.185s] | mae : 0.000733 | mse : 0.000015 | score : -0.00037 |\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      ">  [ info ] restoring from _logs\\2022-08-18_19-45-53-344718\\checkpoints\\model_2024.pt\n",
      "| epoch  -1  [-1 / 8] [0.867s] | mae : 0.000724 | mse : 0.000015 | score : -0.00037 |\n"
     ]
    }
   ],
   "source": [
    "m = cflearn.api.fit_ml(\n",
    "    x,\n",
    "    y,\n",
    "    # specify the `model`\n",
    "    core_name=\"fcnn\",\n",
    "    # specify the `loss`\n",
    "    loss_name=\"mae\",\n",
    "    # specify the `metrics`\n",
    "    metric_names=[\"mae\", \"mse\"],\n",
    "    # some common settings\n",
    "    output_dim=1,\n",
    "    is_classification=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We can see that everything works like a charm and the final result is pretty decent.\n",
    "\n",
    "We can inspect all the modules that `carefree-learn` supports with `supported_ml_models`, `supported_losses` and `supported_metrics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ddr, fcnn, fnet, linear, mixer, mixer_bake, mixer_r_dropout, nbm, ndt, pool_former, rnn, rnn_bake, transformer, wnd'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(cflearn.api.supported_ml_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PLACEHOLDER], adain, bce, corr, cross_entropy, ddr, focal, iou, label_smooth_cross_entropy, mae, mse, quantile, recon, sigmoid_mae, siren_vae, style_vae, vae, vae1d, vae2d, vq_vae'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(cflearn.api.supported_losses())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'acc, auc, aux, ber, corr, f1, iou, mae, mse, quantile, r2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join(cflearn.api.supported_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize Modules\n",
    "\n",
    "`carefree-learn` also supports replacing any module in the workflow. In this section, We will show you how to customize your own models, losses and metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize Models\n",
    "\n",
    "We can utilize `register_ml_module` to register any `nn.Module` into `carefree-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "                                Internal Default Configurations Used by `carefree-learn`                                \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "                             train_samples   |   1000\n",
      "                             valid_samples   |   None\n",
      "                         max_snapshot_file   |   25\n",
      "                                 input_dim   |   10\n",
      "                                 workplace   |   _logs\\2022-08-18_19-46-00-931779\n",
      "                             monitor_names   |   ['mean_std', 'plateau']\n",
      "                      additional_callbacks   |   ['_log_metrics_msg', '_inject_loader_name']\n",
      "                   log_metrics_msg_verbose   |   True\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    MyFancyLinear                           [-1, 10]                                  [-1, 1]                   11\n",
      "      Linear                                [-1, 10]                                  [-1, 1]                   11\n",
      "========================================================================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "| epoch  44  [8 / 8] [0.918s] | mae : 1.583502 | mse : 2.656770 | score : -2.12013 |\n",
      "| epoch  88  [8 / 8] [0.955s] | mae : 0.199840 | mse : 0.061786 | score : -0.13081 |\n",
      "| epoch 132  [8 / 8] [0.967s] | mae : 0.151507 | mse : 0.035672 | score : -0.09358 |\n",
      "| epoch 176  [8 / 8] [1.196s] | mae : 0.094346 | mse : 0.013858 | score : -0.05410 |\n",
      "| epoch 220  [8 / 8] [1.002s] | mae : 0.040393 | mse : 0.002446 | score : -0.02142 |\n",
      "| epoch 264  [8 / 8] [1.733s] | mae : 0.015624 | mse : 0.000371 | score : -0.00799 |\n",
      "| epoch 308  [8 / 8] [1.475s] | mae : 0.000807 | mse : 0.000000 | score : -0.00040 |\n",
      "| epoch 352  [8 / 8] [0.755s] | mae : 0.000000 | mse : 0.000000 | score : -0.00000 |\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      ">  [ info ] restoring from _logs\\2022-08-18_19-46-00-931779\\checkpoints\\model_2880.pt\n",
      "| epoch  -1  [-1 / 8] [0.517s] | mae : 0.000000 | mse : 0.000000 | score : -0.00000 |\n"
     ]
    }
   ],
   "source": [
    "@cflearn.register_ml_module(\"my_fancy_linear\")\n",
    "class MyFancyLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "    \n",
    "    def forward(self, net):\n",
    "        return self.linear(net)\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    x,\n",
    "    y,\n",
    "    # THIS LINE IS CHANGED!\n",
    "    core_name=\"my_fancy_linear\",\n",
    "    loss_name=\"mae\",\n",
    "    metric_names=[\"mae\", \"mse\"],\n",
    "    # some common settings\n",
    "    output_dim=1,\n",
    "    is_classification=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the result is almost perfect now!\n",
    "\n",
    "> It is recommended to go through the [`models`](../models.ipynb) recipe for more details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize Losses\n",
    "\n",
    "We can utilize `register_loss_module` to register any `nn.Module` into `carefree-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "                                Internal Default Configurations Used by `carefree-learn`                                \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "                             train_samples   |   1000\n",
      "                             valid_samples   |   None\n",
      "                         max_snapshot_file   |   25\n",
      "                                 input_dim   |   10\n",
      "                                 workplace   |   _logs\\2022-08-18_19-46-10-486108\n",
      "                             monitor_names   |   ['mean_std', 'plateau']\n",
      "                      additional_callbacks   |   ['_log_metrics_msg', '_inject_loader_name']\n",
      "                   log_metrics_msg_verbose   |   True\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    MyFancyLinear                           [-1, 10]                                  [-1, 1]                   11\n",
      "      Linear                                [-1, 10]                                  [-1, 1]                   11\n",
      "========================================================================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "| epoch  44  [8 / 8] [1.163s] | mae : 0.587211 | mse : 0.435902 | score : -0.51155 |\n",
      "| epoch  88  [8 / 8] [0.956s] | mae : 0.197011 | mse : 0.059929 | score : -0.12847 |\n",
      "| epoch 132  [8 / 8] [1.240s] | mae : 0.141260 | mse : 0.030704 | score : -0.08598 |\n",
      "| epoch 176  [8 / 8] [1.069s] | mae : 0.076938 | mse : 0.008938 | score : -0.04293 |\n",
      "| epoch 220  [8 / 8] [1.082s] | mae : 0.026444 | mse : 0.001061 | score : -0.01375 |\n",
      "| epoch 264  [8 / 8] [1.295s] | mae : 0.005920 | mse : 0.000053 | score : -0.00298 |\n",
      "| epoch 308  [8 / 8] [1.357s] | mae : 0.000012 | mse : 0.000000 | score : -0.00000 |\n",
      "| epoch 352  [8 / 8] [0.873s] | mae : 0.000000 | mse : 0.000000 | score : -0.00000 |\n",
      "| epoch 396  [8 / 8] [1.154s] | mae : 0.000000 | mse : 0.000000 | score : -0.00000 |\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      ">  [ info ] restoring from _logs\\2022-08-18_19-46-10-486108\\checkpoints\\model_2644.pt\n",
      "| epoch  -1  [-1 / 8] [0.675s] | mae : 0.000000 | mse : 0.000000 | score : -0.00000 |\n"
     ]
    }
   ],
   "source": [
    "@cflearn.register_loss_module(\"my_fancy_loss\")\n",
    "class MyFancyLoss(nn.Module):\n",
    "    def forward(self, predictions, target):\n",
    "        return (predictions - target).abs()\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    x,\n",
    "    y,\n",
    "    core_name=\"my_fancy_linear\",\n",
    "    # THIS LINE IS CHANGED!\n",
    "    loss_name=\"my_fancy_loss\",\n",
    "    metric_names=[\"mae\", \"mse\"],\n",
    "    # some common settings\n",
    "    output_dim=1,\n",
    "    is_classification=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the result is also perfect! This is because `my_fancy_loss` is just a hand-written `mae` loss, so the result should not change.\n",
    "\n",
    "> It is recommended to go through the [`losses`](../losses.ipynb) recipe for more details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize Metrics\n",
    "\n",
    "We can utilize `register_metric` & `IMetric` to register any `IMetric`-like object into `carefree-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "                                Internal Default Configurations Used by `carefree-learn`                                \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "                             train_samples   |   1000\n",
      "                             valid_samples   |   None\n",
      "                         max_snapshot_file   |   25\n",
      "                                 input_dim   |   10\n",
      "                                 workplace   |   _logs\\2022-08-18_19-46-21-400019\n",
      "                             monitor_names   |   ['mean_std', 'plateau']\n",
      "                      additional_callbacks   |   ['_log_metrics_msg', '_inject_loader_name']\n",
      "                   log_metrics_msg_verbose   |   True\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    MyFancyLinear                           [-1, 10]                                  [-1, 1]                   11\n",
      "      Linear                                [-1, 10]                                  [-1, 1]                   11\n",
      "========================================================================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "| epoch  44  [8 / 8] [1.279s] | mae : 0.695226 | my_mae : 0.695226 | score : -0.69522 |\n",
      "| epoch  88  [8 / 8] [1.219s] | mae : 0.246233 | my_mae : 0.246233 | score : -0.24623 |\n",
      "| epoch 132  [8 / 8] [1.287s] | mae : 0.187505 | my_mae : 0.187505 | score : -0.18750 |\n",
      "| epoch 176  [8 / 8] [1.027s] | mae : 0.120509 | my_mae : 0.120509 | score : -0.12050 |\n",
      "| epoch 220  [8 / 8] [1.753s] | mae : 0.060345 | my_mae : 0.060345 | score : -0.06034 |\n",
      "| epoch 264  [8 / 8] [1.009s] | mae : 0.030635 | my_mae : 0.030635 | score : -0.03063 |\n",
      "| epoch 308  [8 / 8] [1.015s] | mae : 0.007904 | my_mae : 0.007904 | score : -0.00790 |\n",
      "| epoch 352  [8 / 8] [0.844s] | mae : 0.000003 | my_mae : 0.000003 | score : -0.00000 |\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      ">  [ info ] restoring from _logs\\2022-08-18_19-46-21-400019\\checkpoints\\model_3048.pt\n",
      "| epoch  -1  [-1 / 8] [0.752s] | mae : 0.000000 | my_mae : 0.000000 | score : -0.00000 |\n"
     ]
    }
   ],
   "source": [
    "@cflearn.register_metric(\"my_mae\")\n",
    "class MyMAE(cflearn.IMetric):\n",
    "    # False means that the smaller this metric is, the better.\n",
    "    @property\n",
    "    def is_positive(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    # predictions : [N, 1]\n",
    "    # labels      : [N, 1]\n",
    "    def forward(self, predictions: np.ndarray, labels: np.ndarray) -> float:\n",
    "        return np.abs(predictions - labels).mean().item()\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    x,\n",
    "    y,\n",
    "    core_name=\"my_fancy_linear\",\n",
    "    loss_name=\"my_fancy_loss\",\n",
    "    # THIS LINE IS CHANGED!\n",
    "    metric_names=[\"my_mae\", \"mae\"],\n",
    "    # some common settings\n",
    "    output_dim=1,\n",
    "    is_classification=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `my_mae` always equals to `mae`, which is expected.\n",
    "\n",
    "> It is recommended to go through the [`metrics`](../metrics.ipynb) recipe for more details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Data Processing\n",
    "\n",
    "Although it will be nice to have the dataset already processed, in most real-world cases, we need to handle the data processing ourselves. In `carefree-learn`, there are two ways to handle data processing: out of the loop, or utilize the integrated `IMLData` system.\n",
    "\n",
    "Here's the toy dataset that we'll use through out this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 10), (1000, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 10\n",
    "num_sample = 1000\n",
    "\n",
    "x = np.random.random([num_sample, dim])\n",
    "_x_mean = x.mean(0)\n",
    "_x_std  = x.std(0)\n",
    "_x_normalized = (x - _x_mean) / _x_std\n",
    "w = np.random.random([dim, 1])\n",
    "y = _x_normalized.dot(w)\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that if we normalize the dataset correctly, this task will be exactly the same as the task we've encountered in the [No Data Processing](#No-Data-Processing) case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Without Data Processing\n",
    "\n",
    "In order to demonstrate why data processing is important, let's run an experiment without it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "                                Internal Default Configurations Used by `carefree-learn`                                \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "                             train_samples   |   1000\n",
      "                             valid_samples   |   None\n",
      "                         max_snapshot_file   |   25\n",
      "                                 input_dim   |   10\n",
      "                                 workplace   |   _logs\\2022-08-18_19-46-31-637018\n",
      "                             monitor_names   |   ['mean_std', 'plateau']\n",
      "                      additional_callbacks   |   ['_log_metrics_msg', '_inject_loader_name']\n",
      "                   log_metrics_msg_verbose   |   True\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    MyFancyLinear                           [-1, 10]                                  [-1, 1]                   11\n",
      "      Linear                                [-1, 10]                                  [-1, 1]                   11\n",
      "========================================================================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "| epoch  44  [8 / 8] [0.998s] | mae : 1.528571 | my_mae : 1.528571 | score : -1.52857 |\n",
      "| epoch  88  [8 / 8] [0.980s] | mae : 1.450464 | my_mae : 1.450464 | score : -1.45046 |\n",
      "| epoch 132  [8 / 8] [0.962s] | mae : 1.379952 | my_mae : 1.379952 | score : -1.37995 |\n",
      "| epoch 176  [8 / 8] [1.136s] | mae : 1.314606 | my_mae : 1.314606 | score : -1.31460 |\n",
      "| epoch 220  [8 / 8] [1.088s] | mae : 1.254905 | my_mae : 1.254905 | score : -1.25490 |\n",
      "| epoch 264  [8 / 8] [2.066s] | mae : 1.201328 | my_mae : 1.201328 | score : -1.20132 |\n",
      "| epoch 308  [8 / 8] [1.120s] | mae : 1.152469 | my_mae : 1.152469 | score : -1.15246 |\n",
      "| epoch 352  [8 / 8] [1.118s] | mae : 1.106240 | my_mae : 1.106240 | score : -1.10624 |\n",
      "| epoch 396  [8 / 8] [1.137s] | mae : 1.062136 | my_mae : 1.062136 | score : -1.06213 |\n",
      "| epoch 440  [8 / 8] [1.160s] | mae : 1.018885 | my_mae : 1.018885 | score : -1.01888 |\n",
      "| epoch 484  [8 / 8] [1.142s] | mae : 0.976395 | my_mae : 0.976395 | score : -0.97639 |\n",
      "| epoch 528  [8 / 8] [1.037s] | mae : 0.934531 | my_mae : 0.934531 | score : -0.93453 |\n",
      "| epoch 572  [8 / 8] [1.148s] | mae : 0.893219 | my_mae : 0.893219 | score : -0.89321 |\n",
      "| epoch 616  [8 / 8] [1.059s] | mae : 0.852403 | my_mae : 0.852403 | score : -0.85240 |\n",
      "| epoch 660  [8 / 8] [1.159s] | mae : 0.811511 | my_mae : 0.811511 | score : -0.81151 |\n",
      "| epoch 704  [8 / 8] [1.042s] | mae : 0.770452 | my_mae : 0.770452 | score : -0.77045 |\n",
      "| epoch 748  [8 / 8] [1.074s] | mae : 0.729380 | my_mae : 0.729380 | score : -0.72938 |\n",
      "| epoch 792  [8 / 8] [1.071s] | mae : 0.688423 | my_mae : 0.688423 | score : -0.68842 |\n",
      "| epoch 836  [8 / 8] [0.948s] | mae : 0.647454 | my_mae : 0.647454 | score : -0.64745 |\n",
      "| epoch 880  [8 / 8] [0.974s] | mae : 0.606356 | my_mae : 0.606356 | score : -0.60635 |\n",
      "| epoch 924  [8 / 8] [1.199s] | mae : 0.565287 | my_mae : 0.565287 | score : -0.56528 |\n",
      "| epoch 968  [8 / 8] [1.054s] | mae : 0.524512 | my_mae : 0.524512 | score : -0.52451 |\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      ">  [ info ] restoring from _logs\\2022-08-18_19-46-31-637018\\checkpoints\\model_8000.pt\n",
      "| epoch  -1  [-1 / 8] [0.731s] | mae : 0.494821 | my_mae : 0.494821 | score : -0.49482 |\n"
     ]
    }
   ],
   "source": [
    "m = cflearn.api.fit_ml(\n",
    "    x,\n",
    "    y,\n",
    "    core_name=\"my_fancy_linear\",\n",
    "    loss_name=\"my_fancy_loss\",\n",
    "    metric_names=[\"my_mae\", \"mae\"],\n",
    "    # some common settings\n",
    "    output_dim=1,\n",
    "    is_classification=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the result is much worse than before, although their 'internal' tasks are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of the Loop\n",
    "\n",
    "The most intuitive way is to process the dataset 'out of the loop'. That is, to process the dataset 'somewhere else' before we start using `carefree-learn`. It suffers from certain disadvantages, though, that the final solution can hardly be 'out of the box'. One of the powerful functions that `carefree-learn` supports is that we can save everything into a single `zip` file, so reprodution / deployment will be very easy. But if we process the dataset 'out of the loop', we have to manage to save / load the data processing stuffs, which makes it more difficult to manage the solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilize the Integrated `IMLData` System\n",
    "\n",
    "To make things easier, `carefree-learn` introduces the `IMLData` system. By following some protocols, we can integrate the data processing stuffs into `carefree-learn`, so they can be saved into the same `zip` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Here's a minimal example where we can integrate the normalization process with this system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cflearn import IMLData\n",
    "from cflearn import IMLDataProcessor\n",
    "from cflearn import IMLPreProcessedData\n",
    "from cflearn import register_ml_data\n",
    "from cflearn import register_ml_data_processor\n",
    "\n",
    "\n",
    "@register_ml_data_processor(\"my_fancy_processor\", allow_duplicate=True)\n",
    "class MyFancyProcessor(IMLDataProcessor):\n",
    "    mean: np.ndarray\n",
    "    std: np.ndarray\n",
    "    \n",
    "    def build_with(self, x_train) -> None:\n",
    "        self.mean = x_train.mean(0)\n",
    "        self.std = x_train.std(0)\n",
    "\n",
    "    def preprocess(self, x_train, y_train, x_valid, y_valid) -> IMLPreProcessedData:\n",
    "        x_train = (x_train - self.mean) / self.std\n",
    "        if x_valid is not None:\n",
    "            x_valid = (x_valid - self.mean) / self.std\n",
    "        return IMLPreProcessedData(x_train, y_train, x_valid, y_valid)\n",
    "\n",
    "    def dumps(self):\n",
    "        return {\n",
    "            \"mean\": self.mean.tolist(),\n",
    "            \"std\": self.std.tolist(),\n",
    "        }\n",
    "\n",
    "    def loads(self, dumped) -> None:\n",
    "        self.mean = np.array(dumped[\"mean\"], np.float32)\n",
    "        self.std = np.array(dumped[\"std\"], np.float32)\n",
    "\n",
    "\n",
    "@register_ml_data(\"my_fancy_data\")\n",
    "class MyFancyData(IMLData):\n",
    "    processor_type = \"my_fancy_processor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it seems to be a lot of codes, they should be pretty straight forward to understand. We'll dive into the details in the [Explanations](#Explanations) section, for now let's try it out and see if everything works well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "                                Internal Default Configurations Used by `carefree-learn`                                \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "                             train_samples   |   1000\n",
      "                             valid_samples   |   None\n",
      "                         max_snapshot_file   |   25\n",
      "                                 input_dim   |   10\n",
      "                                 workplace   |   _logs\\2022-08-18_19-46-57-109509\n",
      "                             monitor_names   |   ['mean_std', 'plateau']\n",
      "                      additional_callbacks   |   ['_log_metrics_msg', '_inject_loader_name']\n",
      "                   log_metrics_msg_verbose   |   True\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    MyFancyLinear                           [-1, 10]                                  [-1, 1]                   11\n",
      "      Linear                                [-1, 10]                                  [-1, 1]                   11\n",
      "========================================================================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "| epoch  44  [8 / 8] [1.123s] | mae : 0.883994 | my_mae : 0.883994 | score : -0.88399 |\n",
      "| epoch  88  [8 / 8] [1.215s] | mae : 0.311702 | my_mae : 0.311702 | score : -0.31170 |\n",
      "| epoch 132  [8 / 8] [0.948s] | mae : 0.000480 | my_mae : 0.000480 | score : -0.00048 |\n",
      "| epoch 176  [8 / 8] [0.799s] | mae : 0.000000 | my_mae : 0.000000 | score : -0.00000 |\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      ">  [ info ] restoring from _logs\\2022-08-18_19-46-57-109509\\checkpoints\\model_1396.pt\n",
      "| epoch  -1  [-1 / 8] [0.204s] | mae : 0.000000 | my_mae : 0.000000 | score : -0.00000 |\n"
     ]
    }
   ],
   "source": [
    "m = cflearn.api.fit_ml(\n",
    "    # THIS LINE IS CHANGED!\n",
    "    MyFancyData(x, y),\n",
    "    core_name=\"my_fancy_linear\",\n",
    "    loss_name=\"my_fancy_loss\",\n",
    "    metric_names=[\"my_mae\", \"mae\"],\n",
    "    # some common settings\n",
    "    output_dim=1,\n",
    "    is_classification=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! As we can see, the result becomes perfect again.\n",
    "\n",
    "As we mentioned before, the best advantage of utilizing the `IMLData` system is that, we can save the data processing procedure into the same `zip` file that we use to save our models. For example, by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cflearn.api.ml.pipeline.MLPipeline at 0x1e54471bbe0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.save(\"./test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will generate a `test.zip` file which can be loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m2 = cflearn.api.load(\"./test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use it to make predictions and see if the data processing procedure is preserved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.419603043329165e-07"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idata = m2.make_inference_data(x)\n",
    "predictions = m2.predict(idata)[cflearn.PREDICTIONS_KEY]\n",
    "# calculate the mae\n",
    "np.abs(y - predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the data processing procedure is perfectly preserved! We can also inspect the processed data to consolidate our confidence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.04624677e-07,  8.98624838e-08,  9.77361483e-08,  2.68471631e-08,\n",
       "        -9.40245746e-08, -3.79181705e-08,  5.08937793e-08, -7.58817618e-09,\n",
       "         3.30258817e-08,  3.82606370e-08]),\n",
       " array([0.99999997, 0.99999996, 1.00000002, 1.00000001, 0.99999998,\n",
       "        1.00000001, 0.99999995, 0.99999996, 1.00000002, 1.00000002]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_processed = idata.train_data.x\n",
    "x_processed.mean(0), x_processed.std(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanations\n",
    "\n",
    "To start up, we can first write down these two parts:\n",
    "\n",
    "```python\n",
    "# The name, `xxx_processor`, can be an arbitrary name. Just make sure it is 'unique' so it will not collide with others.\n",
    "@register_ml_data_processor(\"xxx_processor\", allow_duplicate=True)\n",
    "class Processor(IMLDataProcessor):\n",
    "    ...\n",
    "\n",
    "# The name, `xxx_data`, can be an arbitrary name. Just make sure it is 'unique' so it will not collide with others.\n",
    "@register_ml_data(\"xxx_data\")\n",
    "class Data(IMLData):\n",
    "    # make sure that the `processor_type` matches the name you used in the `register_ml_data_processor` above\n",
    "    processor_type = \"xxx_processor\"\n",
    "```\n",
    "\n",
    "These will register a new `IMLDataProcessor` and a new `IMLData` who uses the new `IMLDataProcessor` into `carefree-learn`. After these, we can use them by constructing the training data with the new `IMLData`:\n",
    "\n",
    "```python\n",
    "data = Data(x, y)\n",
    "```\n",
    "\n",
    "We support using validation dataset as well:\n",
    "\n",
    "```python\n",
    "data = Data(x_train, y_train, x_valid, y_valid)\n",
    "```\n",
    "\n",
    "With the `fit_ml` API, we can utilize the `data` easily:\n",
    "\n",
    "```python\n",
    "m = cflearn.api.fit_ml(\n",
    "    data,\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "So the key tasks are: how to define the details in `IMLDataProcessor`? We'll walk through each of its `abstractmethod` in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General\n",
    "\n",
    "In order to reduce the number of boiler plate codes, `carefree-learn` allows you to 'choose' the arguments that you want for your implementations. For example, the `build_with` defined in `IMLDataProcessor` is:\n",
    "\n",
    "```python\n",
    "@abstractmethod\n",
    "def build_with(\n",
    "    self,\n",
    "    config: Dict[str, Any],\n",
    "    x_train: Union[np.ndarray, str],\n",
    "    y_train: Optional[Union[np.ndarray, str]],\n",
    "    x_valid: Optional[Union[np.ndarray, str]],\n",
    "    y_valid: Optional[Union[np.ndarray, str]],\n",
    ") -> None:\n",
    "    pass\n",
    "```\n",
    "\n",
    "But in the `MyFancyProcessor`, we actually wrote:\n",
    "\n",
    "```python\n",
    "def build_with(self, x_train) -> None:\n",
    "    self.mean = x_train.mean(0)\n",
    "    self.std = x_train.std(0)\n",
    "```\n",
    "\n",
    "As you can see, we only 'chose' `x_train` for our implementations. In fact, you can 'choose' any number of arguments based on your actual requirements, without having to write down all five arguments defined in the `IMLDataProcessor` interface!\n",
    "\n",
    "> In the following sections, we will introduce all the arguments defined in the interface, but keep in mind that you don't need to write down all of them in your own implementations, as you can choose what you need!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `build_with`\n",
    "\n",
    "```python\n",
    "@abstractmethod\n",
    "def build_with(\n",
    "    self,\n",
    "    config: Dict[str, Any],\n",
    "    x_train: Union[np.ndarray, str],\n",
    "    y_train: Optional[Union[np.ndarray, str]],\n",
    "    x_valid: Optional[Union[np.ndarray, str]],\n",
    "    y_valid: Optional[Union[np.ndarray, str]],\n",
    ") -> None:\n",
    "    pass\n",
    "```\n",
    "\n",
    "- `config`: configurations specified by the corresponding `IMLData`.\n",
    "  - It will be defined in the `processor_build_config` property, as explained below.\n",
    "- `x_train`: training data, could be `str` when we need to handle file datasets.\n",
    "  - See [`from_file`](./from_file.ipynb) recipe for more details.\n",
    "- `y_train`: training labels, could be `None` if `x_train` is a file or not provided.\n",
    "  - It is common that labels are not provided at inference time. \n",
    "- `x_valid`: validation data, could be `str` when we need to handle file datasets, could be `None` if not provided.\n",
    "- `y_valid`: validation labels, could be `None` if not provided.\n",
    "\n",
    "This method will only be called when we are instantiating a `IMLData` instance. Here's the pseudo codes of the `__init__` process:\n",
    "\n",
    "```python\n",
    "class IMLData:\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_valid,\n",
    "        y_valid,\n",
    "        ...,\n",
    "    ):\n",
    "        ...\n",
    "        # here, we should define `processor_build_config` to pass some configs to our processor\n",
    "        kw = dict(\n",
    "            config=self.processor_build_config,\n",
    "            x_train=x_train,\n",
    "            y_train=y_train,\n",
    "            x_valid=x_valid,\n",
    "            y_valid=y_valid,\n",
    "        )\n",
    "        # `build_with` will be called here\n",
    "        processor.build_with(**kw)\n",
    "    \n",
    "    @property\n",
    "    def processor_build_config(self) -> Dict[str, Any]:\n",
    "        ...\n",
    "        \n",
    "```\n",
    "\n",
    "So if out processor needs to be configured, we need to pass the configurations to our own `IMLData` instance, and then define them in the `processor_build_config` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> x_train    foo.train\n",
      "> config.foo 1.2345\n"
     ]
    }
   ],
   "source": [
    "@register_ml_data_processor(\"foo_processor\", allow_duplicate=True)\n",
    "class FooProcessor(IMLDataProcessor):\n",
    "    def build_with(self, config, x_train):\n",
    "        print(\"> x_train   \", x_train)\n",
    "        print(\"> config.foo\", config[\"foo\"])\n",
    "    \n",
    "    def preprocess(self):\n",
    "        pass\n",
    "    \n",
    "    def dumps(self):\n",
    "        pass\n",
    "    \n",
    "    def loads(self):\n",
    "        pass\n",
    "\n",
    "@register_ml_data(\"foo_data\", allow_duplicate=True)\n",
    "class FooData(IMLData):\n",
    "    processor_type = \"foo_processor\"\n",
    "    \n",
    "    def __init__(self, x_train, foo):\n",
    "        # the extra assignments should take place before the `super` call, because\n",
    "        # `processor_build_config` will be used in the `super` call\n",
    "        self.foo = foo\n",
    "        super().__init__(x_train)\n",
    "    \n",
    "    @property\n",
    "    def processor_build_config(self):\n",
    "        return dict(\n",
    "            foo=self.foo,\n",
    "        )\n",
    "\n",
    "data = FooData(\"foo.train\", 1.2345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `preprocess`\n",
    "\n",
    "```python\n",
    "class IMLPreProcessedData(NamedTuple):\n",
    "    x_train: np.ndarray\n",
    "    y_train: Optional[np.ndarray] = None\n",
    "    x_valid: Optional[np.ndarray] = None\n",
    "    y_valid: Optional[np.ndarray] = None\n",
    "    # if input_dim is not specified, `x_train.shape[-1]` will be used\n",
    "    input_dim: Optional[int] = None\n",
    "    num_history: Optional[int] = None\n",
    "    num_classes: Optional[int] = None\n",
    "    is_classification: Optional[bool] = None\n",
    "\n",
    "@abstractmethod\n",
    "def preprocess(\n",
    "    self,\n",
    "    config: Dict[str, Any],\n",
    "    x_train: Union[np.ndarray, str],\n",
    "    y_train: Optional[Union[np.ndarray, str]],\n",
    "    x_valid: Optional[Union[np.ndarray, str]],\n",
    "    y_valid: Optional[Union[np.ndarray, str]],\n",
    "    *,\n",
    "    for_inference: bool,\n",
    ") -> IMLPreProcessedData:\n",
    "    pass\n",
    "```\n",
    "\n",
    "- `config`: configurations specified by the corresponding `IMLData`.\n",
    "  - It will be defined in the `processor_preprocess_config` property, as explained below.\n",
    "- `x_train`: original training data, could be `str` when we need to handle file datasets.\n",
    "  - See [`from_file`](./from_file.ipynb) recipe for more details.\n",
    "- `y_train`: original training labels, could be `None` if `x_train` is a file or not provided.\n",
    "- `x_valid`: original validation data, could be `None` if not provided.\n",
    "- `y_valid`: original validation labels, could be `None` if not provided.\n",
    "\n",
    "The `preprocess` method should return a `IMLPreProcessedData` namedtuple:\n",
    "- `x_train`: preprocessed training features.\n",
    "- `y_train`: preprocessed training labels, could be `None` if not provided.\n",
    "  - It is common that labels are not provided at inference time.\n",
    "- `x_valid`: preprocessed validation features, could be `None` if not provided.\n",
    "- `y_valid`: preprocessed validation labels, could be `None` if not provided.\n",
    "- `input_dim`: input feature dim that the model will receive.\n",
    "  - If not provided, `x_train.shape[-1]` will be used.\n",
    "  - If `encoder` is used, this setting will not represent the final input dim that your model will receive, because the `encoder` might 'expand' the dimension with some encoding methods.\n",
    "- `num_history`: number of history, useful in time series tasks.\n",
    "  - If not provided, we will use the default value defined in the pipeline.\n",
    "- `num_classes`: number of classes, will be used as `output_dim` if `is_classification` is True & `output_dim` is not specified.\n",
    "  - If not provided, we will use the default value defined in the pipeline.\n",
    "- `is_classification`: whether current task is a classification task.\n",
    "  - If not provided, we will use the default value defined in the pipeline.\n",
    "  \n",
    "This method, as its name suggests, will be called before `IMLData` construct its datasets and dataloaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `dumps` / `loads`\n",
    "\n",
    "```python\n",
    "@abstractmethod\n",
    "def dumps(self) -> Any:\n",
    "    pass\n",
    "\n",
    "@abstractmethod\n",
    "def loads(self, dumped: Any) -> None:\n",
    "    pass\n",
    "```\n",
    "\n",
    "- `dumps`: return an object that holds the necessary information for the `loads` method.\n",
    "- `loads`: setup the processor with the object (`dumped`) returned by the `dumps` method.\n",
    "\n",
    "These two methods are the key parts of the serialization process. When we save / load our pipeline via `m.save` / `cflearn.api.load`, these methods will be called respectively at the proper places.\n",
    "\n",
    "Although almost anything can be handled by `carefree-learn`, following these best practices can make your processor more light-weight and performant:\n",
    "\n",
    "- return a JSON-serializable object in the `dumps` method.\n",
    "- Turn small `np.ndarray` into a python `list` (as we've done in the `MyFancyProcessor`).\n",
    "- Save large `np.ndarray` to a certain path, and then dump the path instead of the `np.ndarray` itself.\n",
    "  - This practice applies to other large objects as well.\n",
    "\n",
    "> For the last suggestion, we should keep in mind that this situation can hardly happen if our processor is defined properly. Because in most cases, a processor should only hold the necessary information for processing the data, so it hardly needs to contain large `np.ndarray`s / objects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Labels\n",
    "\n",
    "You might noticed that the `preprocess` method will take in `y_train` and `y_valid`, and the returned `IMLPreProcessedData` also contains `y_train` and `y_valid`. That's because `carefree-learn` also supports processing labels with the `IMLData` system.\n",
    "\n",
    "Here's the toy dataset that we'll use through out this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 10), (1000, 1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 10\n",
    "num_sample = 1000\n",
    "\n",
    "x = np.random.random([num_sample, dim])\n",
    "w = np.random.random([dim, 1])\n",
    "y = x.dot(w)\n",
    "y = np.exp(y)\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's run an experiment without processing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "                                Internal Default Configurations Used by `carefree-learn`                                \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "                             train_samples   |   1000\n",
      "                             valid_samples   |   None\n",
      "                         max_snapshot_file   |   25\n",
      "                                 input_dim   |   10\n",
      "                                 workplace   |   _logs\\2022-08-18_19-47-01-602958\n",
      "                             monitor_names   |   ['mean_std', 'plateau']\n",
      "                      additional_callbacks   |   ['_log_metrics_msg', '_inject_loader_name']\n",
      "                   log_metrics_msg_verbose   |   True\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    MyFancyLinear                           [-1, 10]                                  [-1, 1]                   11\n",
      "      Linear                                [-1, 10]                                  [-1, 1]                   11\n",
      "========================================================================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "| epoch  44  [8 / 8] [1.044s] | mae : 39.13174 | my_mae : 39.13174 | score : -39.1317 |\n",
      "| epoch  88  [8 / 8] [1.052s] | mae : 37.01518 | my_mae : 37.01518 | score : -37.0151 |\n",
      "| epoch 132  [8 / 8] [0.966s] | mae : 34.89995 | my_mae : 34.89995 | score : -34.8999 |\n",
      "| epoch 176  [8 / 8] [1.164s] | mae : 32.79147 | my_mae : 32.79147 | score : -32.7914 |\n",
      "| epoch 220  [8 / 8] [0.993s] | mae : 30.71376 | my_mae : 30.71376 | score : -30.7137 |\n",
      "| epoch 264  [8 / 8] [0.968s] | mae : 28.73463 | my_mae : 28.73463 | score : -28.7346 |\n",
      "| epoch 308  [8 / 8] [0.925s] | mae : 26.88770 | my_mae : 26.88770 | score : -26.8877 |\n",
      "| epoch 352  [8 / 8] [0.915s] | mae : 25.21847 | my_mae : 25.21847 | score : -25.2184 |\n",
      "| epoch 396  [8 / 8] [0.939s] | mae : 23.75640 | my_mae : 23.75640 | score : -23.7564 |\n",
      "| epoch 440  [8 / 8] [1.216s] | mae : 22.46044 | my_mae : 22.46044 | score : -22.4604 |\n",
      "| epoch 484  [8 / 8] [1.141s] | mae : 21.33755 | my_mae : 21.33755 | score : -21.3375 |\n",
      "| epoch 528  [8 / 8] [0.917s] | mae : 20.34369 | my_mae : 20.34369 | score : -20.3436 |\n",
      "| epoch 572  [8 / 8] [0.926s] | mae : 19.52996 | my_mae : 19.52996 | score : -19.5299 |\n",
      "| epoch 616  [8 / 8] [0.921s] | mae : 18.87097 | my_mae : 18.87097 | score : -18.8709 |\n",
      "| epoch 660  [8 / 8] [0.917s] | mae : 18.35770 | my_mae : 18.35770 | score : -18.3577 |\n",
      "| epoch 704  [8 / 8] [0.918s] | mae : 17.95008 | my_mae : 17.95008 | score : -17.9500 |\n",
      "| epoch 748  [8 / 8] [0.918s] | mae : 17.62307 | my_mae : 17.62307 | score : -17.6230 |\n",
      "| epoch 792  [8 / 8] [0.930s] | mae : 17.36763 | my_mae : 17.36763 | score : -17.3676 |\n",
      "| epoch 836  [8 / 8] [0.989s] | mae : 17.16870 | my_mae : 17.16870 | score : -17.1687 |\n",
      "| epoch 880  [8 / 8] [0.921s] | mae : 17.02361 | my_mae : 17.02361 | score : -17.0236 |\n",
      "| epoch 924  [8 / 8] [0.919s] | mae : 16.92744 | my_mae : 16.92744 | score : -16.9274 |\n",
      "| epoch 968  [8 / 8] [0.917s] | mae : 16.84762 | my_mae : 16.84762 | score : -16.8476 |\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      ">  [ info ] restoring from _logs\\2022-08-18_19-47-01-602958\\checkpoints\\model_8000.pt\n",
      "| epoch  -1  [-1 / 8] [0.677s] | mae : 16.79428 | my_mae : 16.79428 | score : -16.7942 |\n"
     ]
    }
   ],
   "source": [
    "m = cflearn.api.fit_ml(\n",
    "    x,\n",
    "    y,\n",
    "    core_name=\"my_fancy_linear\",\n",
    "    loss_name=\"my_fancy_loss\",\n",
    "    metric_names=[\"my_mae\", \"mae\"],\n",
    "    # some common settings\n",
    "    output_dim=1,\n",
    "    is_classification=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the internal task never changes, the result is not satisfying. Let's define a processor to overcome this issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "                                Internal Default Configurations Used by `carefree-learn`                                \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "                             train_samples   |   1000\n",
      "                             valid_samples   |   None\n",
      "                         max_snapshot_file   |   25\n",
      "                                 input_dim   |   10\n",
      "                                 workplace   |   _logs\\2022-08-18_19-47-23-857891\n",
      "                             monitor_names   |   ['mean_std', 'plateau']\n",
      "                      additional_callbacks   |   ['_log_metrics_msg', '_inject_loader_name']\n",
      "                   log_metrics_msg_verbose   |   True\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    MyFancyLinear                           [-1, 10]                                  [-1, 1]                   11\n",
      "      Linear                                [-1, 10]                                  [-1, 1]                   11\n",
      "========================================================================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "| epoch  44  [8 / 8] [0.913s] | mae : 1.544259 | my_mae : 1.544259 | score : -1.54425 |\n",
      "| epoch  88  [8 / 8] [0.941s] | mae : 0.280699 | my_mae : 0.280699 | score : -0.28069 |\n",
      "| epoch 132  [8 / 8] [0.962s] | mae : 0.235813 | my_mae : 0.235813 | score : -0.23581 |\n",
      "| epoch 176  [8 / 8] [0.929s] | mae : 0.187429 | my_mae : 0.187429 | score : -0.18742 |\n",
      "| epoch 220  [8 / 8] [0.925s] | mae : 0.137892 | my_mae : 0.137892 | score : -0.13789 |\n",
      "| epoch 264  [8 / 8] [0.935s] | mae : 0.097737 | my_mae : 0.097737 | score : -0.09773 |\n",
      "| epoch 308  [8 / 8] [1.038s] | mae : 0.070178 | my_mae : 0.070178 | score : -0.07017 |\n",
      "| epoch 352  [8 / 8] [0.964s] | mae : 0.045920 | my_mae : 0.045920 | score : -0.04592 |\n",
      "| epoch 396  [8 / 8] [0.962s] | mae : 0.019239 | my_mae : 0.019239 | score : -0.01923 |\n",
      "| epoch 440  [8 / 8] [0.864s] | mae : 0.000108 | my_mae : 0.000108 | score : -0.00010 |\n",
      "| epoch 484  [8 / 8] [0.737s] | mae : 0.000000 | my_mae : 0.000000 | score : -0.00000 |\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      ">  [ info ] restoring from _logs\\2022-08-18_19-47-23-857891\\checkpoints\\model_3816.pt\n",
      "| epoch  -1  [-1 / 8] [0.284s] | mae : 0.000000 | my_mae : 0.000000 | score : -0.00000 |\n"
     ]
    }
   ],
   "source": [
    "@register_ml_data_processor(\"my_fancy_label_processor\", allow_duplicate=True)\n",
    "class MyFancyLabelProcessor(IMLDataProcessor):\n",
    "    def build_with(self):\n",
    "        pass\n",
    "\n",
    "    def preprocess(self, x_train, y_train, x_valid, y_valid):\n",
    "        if y_train is not None:\n",
    "            y_train = np.log(y_train)\n",
    "        if y_valid is not None:\n",
    "            y_valid = np.log(y_valid)\n",
    "        return IMLPreProcessedData(x_train, y_train, x_valid, y_valid)\n",
    "    \n",
    "    def dumps(self):\n",
    "        pass\n",
    "    \n",
    "    def loads(self):\n",
    "        pass\n",
    "\n",
    "@register_ml_data(\"my_fancy_label_data\", allow_duplicate=True)\n",
    "class MyFancyLabelData(IMLData):\n",
    "    processor_type = \"my_fancy_label_processor\"\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    # THIS LINE IS CHANGED!\n",
    "    MyFancyLabelData(x, y),\n",
    "    core_name=\"my_fancy_linear\",\n",
    "    loss_name=\"my_fancy_loss\",\n",
    "    metric_names=[\"my_mae\", \"mae\"],\n",
    "    # some common settings\n",
    "    output_dim=1,\n",
    "    is_classification=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! As we can see, the result becomes perfect again. Let's make an inference to ensure everything works well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.12670601237174"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idata = m.make_inference_data(x)\n",
    "predictions = m.predict(idata)[cflearn.PREDICTIONS_KEY]\n",
    "# calculate the mae\n",
    "np.abs(y - predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! The result looks problematic! That's because we only defined processing methods in the `preprocess` method, but we never tell our processor that we need to 'postprocess' the inference results!\n",
    "\n",
    "In order tackle this problem, `carefree-learn` provides a `postprocess_results` callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "                                Internal Default Configurations Used by `carefree-learn`                                \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "                             train_samples   |   1000\n",
      "                             valid_samples   |   None\n",
      "                         max_snapshot_file   |   25\n",
      "                                 input_dim   |   10\n",
      "                                 workplace   |   _logs\\2022-08-18_19-47-34-369372\n",
      "                             monitor_names   |   ['mean_std', 'plateau']\n",
      "                      additional_callbacks   |   ['_log_metrics_msg', '_inject_loader_name']\n",
      "                   log_metrics_msg_verbose   |   True\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    MyFancyLinear                           [-1, 10]                                  [-1, 1]                   11\n",
      "      Linear                                [-1, 10]                                  [-1, 1]                   11\n",
      "========================================================================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "| epoch  44  [8 / 8] [1.815s] | mae : 1.310738 | my_mae : 1.310738 | score : -1.31073 |\n",
      "| epoch  88  [8 / 8] [0.939s] | mae : 0.230377 | my_mae : 0.230377 | score : -0.23037 |\n",
      "| epoch 132  [8 / 8] [0.922s] | mae : 0.188526 | my_mae : 0.188526 | score : -0.18852 |\n",
      "| epoch 176  [8 / 8] [0.922s] | mae : 0.142626 | my_mae : 0.142626 | score : -0.14262 |\n",
      "| epoch 220  [8 / 8] [1.056s] | mae : 0.101454 | my_mae : 0.101454 | score : -0.10145 |\n",
      "| epoch 264  [8 / 8] [0.919s] | mae : 0.073206 | my_mae : 0.073206 | score : -0.07320 |\n",
      "| epoch 308  [8 / 8] [0.921s] | mae : 0.050519 | my_mae : 0.050519 | score : -0.05051 |\n",
      "| epoch 352  [8 / 8] [0.928s] | mae : 0.026560 | my_mae : 0.026560 | score : -0.02656 |\n",
      "| epoch 396  [8 / 8] [0.914s] | mae : 0.000461 | my_mae : 0.000461 | score : -0.00046 |\n",
      "| epoch 440  [8 / 8] [0.705s] | mae : 0.000000 | my_mae : 0.000000 | score : -0.00000 |\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      ">  [ info ] restoring from _logs\\2022-08-18_19-47-34-369372\\checkpoints\\model_3612.pt\n",
      "| epoch  -1  [-1 / 8] [0.541s] | mae : 0.000000 | my_mae : 0.000000 | score : -0.00000 |\n"
     ]
    }
   ],
   "source": [
    "@register_ml_data_processor(\"my_fancy_label_processor\", allow_duplicate=True)\n",
    "class MyFancyLabelProcessor(IMLDataProcessor):\n",
    "    def build_with(self):\n",
    "        pass\n",
    "\n",
    "    def preprocess(self, x_train, y_train, x_valid, y_valid):\n",
    "        if y_train is not None:\n",
    "            y_train = np.log(y_train)\n",
    "        if y_valid is not None:\n",
    "            y_valid = np.log(y_valid)\n",
    "        return IMLPreProcessedData(x_train, y_train, x_valid, y_valid)\n",
    "    \n",
    "    def dumps(self):\n",
    "        pass\n",
    "    \n",
    "    def loads(self):\n",
    "        pass\n",
    "    \n",
    "    def postprocess_results(self, forward):\n",
    "        y = forward[cflearn.PREDICTIONS_KEY]\n",
    "        y = np.exp(y)\n",
    "        forward[cflearn.PREDICTIONS_KEY] = y\n",
    "        return forward\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    # THIS LINE IS CHANGED!\n",
    "    MyFancyLabelData(x, y),\n",
    "    core_name=\"my_fancy_linear\",\n",
    "    loss_name=\"my_fancy_loss\",\n",
    "    metric_names=[\"my_mae\", \"mae\"],\n",
    "    # some common settings\n",
    "    output_dim=1,\n",
    "    is_classification=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an inference again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.831679950732749e-06"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idata = m.make_inference_data(x)\n",
    "predictions = m.predict(idata)[cflearn.PREDICTIONS_KEY]\n",
    "# calculate the mae\n",
    "np.abs(y - predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now our predictions are 'postprocessed' as expected!\n",
    "\n",
    "> More details of `postprocess_results` will be covered in the [Optional Callbacks](#postprocess_results) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try serializations as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.831679950732749e-06"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.save(\"./test\")\n",
    "m2 = cflearn.api.load(\"./test\")\n",
    "idata = m2.make_inference_data(x)\n",
    "predictions = m2.predict(idata)[cflearn.PREDICTIONS_KEY]\n",
    "# calculate the mae\n",
    "np.abs(y - predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the data processing procedure is perfectly preserved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Callbacks\n",
    "\n",
    "Besides the above processing methods, the `IMLData` system also provides several useful callbacks in order to enable the full control of the data flow. We will introduce these callbacks in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_num_samples`\n",
    "\n",
    "```python\n",
    "def get_num_samples(self, x: np.ndarray) -> Optional[int]:\n",
    "    return None\n",
    "```\n",
    "\n",
    "This method can override how the number of samples is calculated. `len(x)` will be used if `None` is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `fetch_batch`\n",
    "\n",
    "```python\n",
    "class IMLBatch(NamedTuple):\n",
    "    input: np.ndarray\n",
    "    labels: Optional[np.ndarray]\n",
    "    others: Optional[np_dict_type] = None\n",
    "\n",
    "def fetch_batch(\n",
    "    self,\n",
    "    x: np.ndarray,\n",
    "    y: Optional[np.ndarray],\n",
    "    indices: Union[int, List[int], np.ndarray],\n",
    ") -> IMLBatch:\n",
    "    return IMLBatch(x[indices], None if y is None else y[indices])\n",
    "```\n",
    "\n",
    "This method defines how the batch will be fetched. The `others` field allows you to inject some additional data to your batch, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "                                Internal Default Configurations Used by `carefree-learn`                                \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "                             train_samples   |   1000\n",
      "                             valid_samples   |   None\n",
      "                         max_snapshot_file   |   25\n",
      "                                 input_dim   |   10\n",
      "                                 workplace   |   _logs\\2022-08-18_19-47-45-463111\n",
      "                             monitor_names   |   ['mean_std', 'plateau']\n",
      "                      additional_callbacks   |   ['_log_metrics_msg', '_inject_loader_name']\n",
      "                   log_metrics_msg_verbose   |   True\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "> foo\n",
      "tensor([[1.2340]])\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    InjectFooModel                                                                                                \n",
      "      Linear                                [-1, 10]                                  [-1, 1]                   11\n",
      "========================================================================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "> foo\n",
      "tensor([[1.2340],\n",
      "        [1.2340],\n",
      "        [1.2340]])\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      "> [warning] no model file found in _logs\\2022-08-18_19-47-45-463111\\checkpoints\n",
      "> foo\n",
      "tensor([[1.2340],\n",
      "        [1.2340],\n",
      "        [1.2340]])\n",
      "| epoch  -1  [-1 / 8] [0.015s] | mae : 47.14639 | my_mae : 47.14639 | score : -47.1463 |\n"
     ]
    }
   ],
   "source": [
    "from cflearn import IMLBatch\n",
    "\n",
    "@register_ml_data_processor(\"inject_foo_processor\", allow_duplicate=True)\n",
    "class InjectFooProcessor(IMLDataProcessor):\n",
    "    def build_with(self):\n",
    "        pass\n",
    "\n",
    "    def preprocess(self, x_train, y_train):\n",
    "        return IMLPreProcessedData(x_train, y_train)\n",
    "    \n",
    "    def dumps(self):\n",
    "        pass\n",
    "    \n",
    "    def loads(self):\n",
    "        pass\n",
    "    \n",
    "    def fetch_batch(self, x, y, indices):\n",
    "        foo = np.full([len(indices), 1], 1.234)\n",
    "        return IMLBatch(\n",
    "            x[indices],\n",
    "            None if y is None else y[indices],\n",
    "            others={\"foo\": foo},\n",
    "        )\n",
    "\n",
    "@register_ml_data(\"inject_foo_data\", allow_duplicate=True)\n",
    "class InjectFooData(IMLData):\n",
    "    processor_type = \"inject_foo_processor\"\n",
    "\n",
    "@cflearn.register_ml_module(\"inject_foo_model\", allow_duplicate=True)\n",
    "class InjectFooModel(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        print(\"> foo\")\n",
    "        print(batch[\"foo\"][:3])\n",
    "        return self.linear(batch[cflearn.INPUT_KEY])\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    InjectFooData(x, y),\n",
    "    core_name=\"inject_foo_model\",\n",
    "    loss_name=\"my_fancy_loss\",\n",
    "    metric_names=[\"my_mae\", \"mae\"],\n",
    "    # some common settings\n",
    "    output_dim=1,\n",
    "    is_classification=False,\n",
    "    # debug\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `postprocess_batch`\n",
    "\n",
    "```python\n",
    "# changes can happen inplace\n",
    "def postprocess_batch(self, batch: np_dict_type) -> np_dict_type:\n",
    "    return batch\n",
    "```\n",
    "\n",
    "This method allows you to postprocess the batch.\n",
    "> In most cases, specifying `fetch_batch` is already enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `postprocess_results`\n",
    "\n",
    "```python\n",
    "# changes can happen inplace\n",
    "def postprocess_results(\n",
    "    self,\n",
    "    forward: np_dict_type,\n",
    "    *,\n",
    "    return_classes: bool,\n",
    "    binary_threshold: float,\n",
    "    return_probabilities: bool,\n",
    ") -> np_dict_type:\n",
    "    return forward\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method allows you to postprocess the inference results.\n",
    "> Notice that:\n",
    "> - the arguments of this method is 'optional'.\n",
    "> - this will only affect the inference methods (e.g. `predict`) and will not affect the training process.\n",
    "\n",
    "- `forward`: the 'raw' inference results.\n",
    "- `return_classes`: whether we need to return class labels instead of the 'raw' results (e.g. logits).\n",
    "- `binary_threshold`: threshold used in binary classification tasks.\n",
    "- `return_probabilities`: whether we need to return the probability predictions.\n",
    "\n",
    "Although there are some flags in this method (`return_classes`, etc.), they are just telling you what kinds of data `forward` will hold, and do not require you to handle them because they have already been handled. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "                                Internal Default Configurations Used by `carefree-learn`                                \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "                             train_samples   |   100\n",
      "                             valid_samples   |   None\n",
      "                         max_snapshot_file   |   25\n",
      "                                 input_dim   |   5\n",
      "                                 loss_name   |   focal\n",
      "                                 workplace   |   _logs\\2022-08-18_19-47-45-508937\n",
      "                             monitor_names   |   ['mean_std', 'plateau']\n",
      "                      additional_callbacks   |   ['_log_metrics_msg', '_inject_loader_name']\n",
      "                   log_metrics_msg_verbose   |   True\n",
      "                              metric_names   |   ['acc', 'auc']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MLModel                                                                                                           \n",
      "  _                                                                                                               \n",
      "    Linear                                   [-1, 5]                                  [-1, 2]                   12\n",
      "      Linear                                 [-1, 5]                                  [-1, 2]                   12\n",
      "========================================================================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "| epoch 350  [1 / 1] [1.853s] | acc : 0.820000 | auc : 0.888799 | score : 0.854399 |\n",
      "| epoch 700  [1 / 1] [1.208s] | acc : 0.960000 | auc : 0.995584 | score : 0.977792 |\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      ">  [ info ] restoring from _logs\\2022-08-18_19-47-45-508937\\checkpoints\\model_671.pt\n",
      "| epoch  -1  [-1 / 1] [0.003s] | acc : 0.960000 | auc : 0.995584 | score : 0.977792 |\n"
     ]
    }
   ],
   "source": [
    "@register_ml_data_processor(\"inspect_processor\", allow_duplicate=True)\n",
    "class InspectProcessor(IMLDataProcessor):\n",
    "    def build_with(self):\n",
    "        pass\n",
    "\n",
    "    def preprocess(self, x_train, y_train):\n",
    "        return IMLPreProcessedData(x_train, y_train)\n",
    "    \n",
    "    def dumps(self):\n",
    "        pass\n",
    "    \n",
    "    def loads(self):\n",
    "        pass\n",
    "    \n",
    "    def postprocess_results(self, forward, *, return_classes, binary_threshold, return_probabilities):\n",
    "        print(\"> return_classes\", return_classes)\n",
    "        print(\"> binary_threshold\", binary_threshold)\n",
    "        print(\"> return_probabilities\", return_probabilities)\n",
    "        print(forward[cflearn.PREDICTIONS_KEY][:3])\n",
    "        return forward\n",
    "\n",
    "@register_ml_data(\"inspect_data\", allow_duplicate=True)\n",
    "class InspectData(IMLData):\n",
    "    processor_type = \"inspect_processor\"\n",
    "\n",
    "\n",
    "x = np.random.randn(100, 5)\n",
    "w = np.random.randn(5, 1)\n",
    "y = (x.dot(w) > 0.0).astype(int)\n",
    "\n",
    "m = cflearn.api.fit_ml(\n",
    "    InspectData(x, y),\n",
    "    core_name=\"linear\",\n",
    "    # some common settings\n",
    "    output_dim=2,\n",
    "    is_classification=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> return_classes False\n",
      "> binary_threshold 0.5\n",
      "> return_probabilities False\n",
      "[[0.3514592  0.4325828 ]\n",
      " [0.78386474 0.3538649 ]\n",
      " [0.46501547 0.35370737]]\n"
     ]
    }
   ],
   "source": [
    "idata = m.make_inference_data(x)\n",
    "predictions = m.predict(idata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> return_classes True\n",
      "> binary_threshold 0.5\n",
      "> return_probabilities False\n",
      "[[1]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "predictions = m.predict(idata, return_classes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> return_classes False\n",
      "> binary_threshold 0.5\n",
      "> return_probabilities True\n",
      "[[0.47973022 0.52026975]\n",
      " [0.60587364 0.3941264 ]\n",
      " [0.5277983  0.47220165]]\n"
     ]
    }
   ],
   "source": [
    "predictions = m.predict(idata, return_probabilities=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, when it comes to the `postprocess_results` method, the `forward` is already processed with those flags, so we can focus on the 'real' postprocess procudure in the `postprocess_results` method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
