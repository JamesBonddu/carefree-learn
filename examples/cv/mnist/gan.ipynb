{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST GAN\n",
    "\n",
    "The MNIST dataset is a dataset of handwritten digits that is commonly used as the 'Hello World' dataset in Deep Learning domain. It contains 60,000 training images and 10,000 testing images, and\n",
    "`carefree-learn` provided a straightforward API to access it.\n",
    "\n",
    "MNIST dataset can be used for training various image processing systems. In this article, we will focus on how to build our custom models to solve the Generative Adversarial Network (GAN) task on MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2082bfdf1b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import cflearn\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Callable\n",
    "from typing import Optional\n",
    "from torch.optim import Optimizer\n",
    "from cflearn.types import tensor_dict_type\n",
    "from cflearn.protocol import IDataLoader\n",
    "from cflearn.protocol import StepOutputs\n",
    "from cflearn.protocol import TrainerState\n",
    "from cflearn.protocol import MetricsOutputs\n",
    "from cflearn.constants import INPUT_KEY\n",
    "from cflearn.constants import PREDICTIONS_KEY\n",
    "from cflearn.misc.toolkit import to_device\n",
    "from cflearn.misc.toolkit import interpolate\n",
    "from cflearn.misc.toolkit import toggle_optimizer\n",
    "from cflearn.modules.blocks import Lambda\n",
    "from cflearn.modules.blocks import UpsampleConv2d\n",
    "from torch.cuda.amp.grad_scaler import GradScaler\n",
    "\n",
    "\n",
    "# MNIST dataset could be prepared with this one line of code\n",
    "data = cflearn.cv.MNISTData(batch_size=16, transform=\"for_generation\")\n",
    "\n",
    "# for reproduction\n",
    "np.random.seed(142857)\n",
    "torch.manual_seed(142857)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the MNIST dataset could be easily turned into a `DLDataModule` instance, which is the common data interface used in `carefree-learn`.\n",
    "\n",
    "> The `transform` argument specifies which transform do we want to use to pre-process the input batch. See [`Transforms`](https://carefree0910.me/carefree-learn-doc/docs/user-guides/computer-vision#transforms) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "\n",
    "For demo purpose, we are going to build a simple convolution-based GAN. But first, let's build the loss function of GAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    def __init__(self):  # type: ignore\n",
    "        super().__init__()\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        self.register_buffer(\"real_label\", torch.tensor(1.0))\n",
    "        self.register_buffer(\"fake_label\", torch.tensor(0.0))\n",
    "\n",
    "    def expand_target(self, tensor: Tensor, use_real_label: bool) -> Tensor:\n",
    "        target = self.real_label if use_real_label else self.fake_label\n",
    "        return target.expand_as(tensor)  # type: ignore\n",
    "\n",
    "    def forward(self, predictions: Tensor, use_real_label: bool) -> Tensor:\n",
    "        target_tensor = self.expand_target(predictions, use_real_label)\n",
    "        loss = self.loss(predictions, target_tensor)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the concept of GAN is fairly easy, it's pretty complicated if we want to implement it with a 'pre-defined' framework. In order to provide full flexibility, `carefree-learn` exposed two methods for users:\n",
    "+ `train_step`, which is used to control **ALL** training behaviours, including:\n",
    "  + calculate losses\n",
    "  + apply back propagation\n",
    "  + perform [automatic mixed precision](https://pytorch.org/docs/stable/amp.html), [gradient norm clipping](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html) and so on\n",
    "+ `evaluate_step`, which is used to define the final metric that we want to monitor.\n",
    "\n",
    "Besides, we also need to define the `forward` method, as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cflearn.register_custom_module(\"simple_gan\")\n",
    "class SimpleGAN(cflearn.CustomModule):\n",
    "    def __init__(self, in_channels: int, img_size: int, latent_dim: int):\n",
    "        super().__init__()\n",
    "        if not latent_dim % 16 == 0:\n",
    "            raise ValueError(f\"`latent_dim` ({latent_dim}) should be divided by 16\")\n",
    "        self.latent_dim = latent_dim\n",
    "        latent_channels = latent_dim // 16\n",
    "        self.generator = nn.Sequential(\n",
    "            Lambda(lambda t: t.view(-1, latent_channels, 4, 4), name=\"reshape\"),\n",
    "            nn.Conv2d(latent_channels, 128, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            UpsampleConv2d(128, 64, kernel_size=3, padding=1, factor=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            UpsampleConv2d(64, 32, kernel_size=3, padding=1, factor=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            UpsampleConv2d(32, in_channels, kernel_size=3, padding=1, factor=2),\n",
    "            Lambda(lambda t: interpolate(t, size=img_size, mode=\"bilinear\")),\n",
    "        )\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "        )\n",
    "        self.loss = GANLoss()\n",
    "\n",
    "    def train_step(\n",
    "        self,\n",
    "        batch_idx: int,\n",
    "        batch: tensor_dict_type,\n",
    "        optimizers: Dict[str, Optimizer],\n",
    "        use_amp: bool,\n",
    "        grad_scaler: GradScaler,\n",
    "        clip_norm_fn: Callable[[], None],\n",
    "        scheduler_step_fn: Callable[[], None],\n",
    "        trainer: cflearn.ITrainer,\n",
    "        forward_kwargs: Dict[str, Any],\n",
    "        loss_kwargs: Dict[str, Any],\n",
    "    ) -> StepOutputs:\n",
    "        net = batch[INPUT_KEY]\n",
    "        # we will explain where do these keys come from in the following markdown block\n",
    "        opt_g = optimizers[\"core.g_parameters\"]\n",
    "        opt_d = optimizers[\"core.d_parameters\"]\n",
    "        # generator step\n",
    "        toggle_optimizer(self, opt_g)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            sampled = self.sample(len(net))\n",
    "            pred_fake = self.discriminator(sampled)\n",
    "            g_loss = self.loss(pred_fake, use_real_label=True)\n",
    "        grad_scaler.scale(g_loss).backward()\n",
    "        clip_norm_fn()\n",
    "        grad_scaler.step(opt_g)\n",
    "        grad_scaler.update()\n",
    "        opt_g.zero_grad()\n",
    "        # discriminator step\n",
    "        toggle_optimizer(self, opt_d)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            pred_real = self.discriminator(net)\n",
    "            loss_d_real = self.loss(pred_real, use_real_label=True)\n",
    "            pred_fake = self.discriminator(sampled.detach().clone())\n",
    "            loss_d_fake = self.loss(pred_fake, use_real_label=False)\n",
    "            d_loss = 0.5 * (loss_d_fake + loss_d_real)\n",
    "        grad_scaler.scale(d_loss).backward()\n",
    "        clip_norm_fn()\n",
    "        grad_scaler.step(opt_d)\n",
    "        grad_scaler.update()\n",
    "        opt_d.zero_grad()\n",
    "        # finalize\n",
    "        scheduler_step_fn()\n",
    "        forward_results = {PREDICTIONS_KEY: sampled}\n",
    "        loss_dict = {\n",
    "            \"g\": g_loss.item(),\n",
    "            \"d\": d_loss.item(),\n",
    "            \"d_fake\": loss_d_fake.item(),\n",
    "            \"d_real\": loss_d_real.item(),\n",
    "        }\n",
    "        return StepOutputs(forward_results, loss_dict)\n",
    "\n",
    "    def evaluate_step(\n",
    "        self,\n",
    "        loader: IDataLoader,\n",
    "        portion: float,\n",
    "        weighted_loss_score_fn: Callable[[Dict[str, float]], float],\n",
    "        trainer: cflearn.ITrainer,\n",
    "    ) -> MetricsOutputs:\n",
    "        loss_items: Dict[str, List[float]] = {}\n",
    "        for i, batch in enumerate(loader):\n",
    "            if i / len(loader) >= portion:\n",
    "                break\n",
    "            batch = to_device(batch, self.device)\n",
    "            net = batch[INPUT_KEY]\n",
    "            sampled = self.sample(len(net))\n",
    "            pred_fake = self.discriminator(sampled)\n",
    "            g_loss = self.loss(pred_fake, use_real_label=True)\n",
    "            pred_real = self.discriminator(net)\n",
    "            d_loss = self.loss(pred_real, use_real_label=True)\n",
    "            loss_items.setdefault(\"g\", []).append(g_loss.item())\n",
    "            loss_items.setdefault(\"d\", []).append(d_loss.item())\n",
    "        # gather\n",
    "        mean_loss_items = {k: sum(v) / len(v) for k, v in loss_items.items()}\n",
    "        mean_loss_items[cflearn.LOSS_KEY] = sum(mean_loss_items.values())\n",
    "        score = weighted_loss_score_fn(mean_loss_items)\n",
    "        return MetricsOutputs(score, mean_loss_items)\n",
    "\n",
    "    @property\n",
    "    def g_parameters(self) -> List[nn.Parameter]:\n",
    "        return list(self.generator.parameters())\n",
    "\n",
    "    @property\n",
    "    def d_parameters(self) -> List[nn.Parameter]:\n",
    "        return list(self.discriminator.parameters())\n",
    "\n",
    "    def sample(self, num_samples: int) -> Tensor:\n",
    "        z = torch.randn(num_samples, self.latent_dim, device=self.device)\n",
    "        return self.generator(z)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch_idx: int,\n",
    "        batch: tensor_dict_type,\n",
    "        state: Optional[TrainerState] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> tensor_dict_type:\n",
    "        return {PREDICTIONS_KEY: self.sample(len(batch[INPUT_KEY]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leveraged the `register_custom_module` API here, which can turn a general `CustomModule` instance to a [`IDLModel`](https://carefree0910.me/carefree-learn-doc/docs/design-principles/#model) in `carefree-learn`. After registered, it can be easily accessed with its name (`\"simple_gan\"`).\n",
    "\n",
    "There are two more things that are worth mentioning:\n",
    "+ When using models with custom steps, we don't need to specify `loss_name` anymore, because the losses are calculated inside `train_step`.\n",
    "+ The `register_custom_module` API will generate a [`IDLModel`](https://carefree0910.me/carefree-learn-doc/docs/design-principles/#model), whose `core` property points to the original `CustomModule`. From the above codes, we can see that `SimpleGAN` implements `g_parameters` and `d_parameters`, which means the `self.core.g_parameters` and `self.core.d_parameters` of the generated [`IDLModel`](https://carefree0910.me/carefree-learn-doc/docs/design-principles/#model) will be two sets of parameters that we wish to optimize.\n",
    "  + In this case, the `core.g_parameter` and `core.d_parameters` will be the optimize `scope` of the generated [`IDLModel`](https://carefree0910.me/carefree-learn-doc/docs/design-principles/#model). That's why we access the optimizers with them.\n",
    "  + Please refer to the [documentation](https://carefree0910.me/carefree-learn-doc/docs/getting-started/configurations#optimizerpack) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "_                                                                                                                 \n",
      "  SimpleGAN                                                                                                       \n",
      "    Sequential-0                           [-1, 128]                          [-1, 1, 28, 28]               94,145\n",
      "      Lambda-0                             [-1, 128]                            [-1, 8, 4, 4]                    0\n",
      "      Conv2d                           [-1, 8, 4, 4]                          [-1, 128, 4, 4]                1,152\n",
      "      ReLU-0                         [-1, 128, 4, 4]                          [-1, 128, 4, 4]                    0\n",
      "      BatchNorm2d-0                  [-1, 128, 4, 4]                          [-1, 128, 4, 4]                  256\n",
      "      UpsampleConv2d-0               [-1, 128, 4, 4]                           [-1, 64, 8, 8]               73,792\n",
      "        Interpolate                  [-1, 128, 4, 4]                          [-1, 128, 8, 8]                    0\n",
      "      ReLU-1                          [-1, 64, 8, 8]                           [-1, 64, 8, 8]                    0\n",
      "      BatchNorm2d-1                   [-1, 64, 8, 8]                           [-1, 64, 8, 8]                  128\n",
      "      UpsampleConv2d-1                [-1, 64, 8, 8]                         [-1, 32, 16, 16]               18,464\n",
      "        Interpolate                   [-1, 64, 8, 8]                         [-1, 64, 16, 16]                    0\n",
      "      ReLU-2                        [-1, 32, 16, 16]                         [-1, 32, 16, 16]                    0\n",
      "      BatchNorm2d-2                 [-1, 32, 16, 16]                         [-1, 32, 16, 16]                   64\n",
      "      UpsampleConv2d-2              [-1, 32, 16, 16]                          [-1, 1, 32, 32]                  289\n",
      "        Interpolate                 [-1, 32, 16, 16]                         [-1, 32, 32, 32]                    0\n",
      "      Lambda-1                       [-1, 1, 32, 32]                          [-1, 1, 28, 28]                    0\n",
      "========================================================================================================================\n",
      "Total params: 191,521\n",
      "Trainable params: 191,521\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.79\n",
      "Params size (MB): 0.73\n",
      "Estimated Total Size (MB): 1.52\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "| epoch  1   [1000 / 3750] [19.882s] | d : 0.386031 | g : 7.567262 | loss : 7.953293 | score : -7.95329 |\n",
      "| epoch  1   [2000 / 3750] [16.676s] | d : 1.988238 | g : 8.908347 | loss : 10.89658 | score : -10.8965 |\n",
      "| epoch  1   [3000 / 3750] [16.733s] | d : 0.864289 | g : 10.74184 | loss : 11.60612 | score : -11.6061 |\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      ">  [ info ] restoring from _logs\\2021-10-27_22-37-03-907273\\checkpoints\\model_1000.pt\n",
      "| epoch  -1  [  -1 / 3750] [12.971s] | d : 0.386031 | g : 7.536458 | loss : 7.922490 | score : -7.92249 |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cflearn.api.cv.pipeline.CarefreePipeline at 0x2082be94390>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that we don't need to explicitly specify `loss_name`!\n",
    "cflearn.api.fit_cv(\n",
    "    data,\n",
    "    \"simple_gan\",\n",
    "    {\"in_channels\": 1, \"img_size\": 28, \"latent_dim\": 128},\n",
    "    optimizer_settings={\n",
    "        \"core.g_parameters\": {\n",
    "            \"optimizer\": \"adam\",\n",
    "            \"scheduler\": \"warmup\",\n",
    "        },\n",
    "        \"core.d_parameters\": {\n",
    "            \"optimizer\": \"adam\",\n",
    "            \"scheduler\": \"warmup\",\n",
    "        },\n",
    "    },\n",
    "    fixed_epoch=1,                                  # for demo purpose, we only train our model for 1 epoch\n",
    "    cuda=0 if torch.cuda.is_available() else None,  # use CUDA if possible\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cflearn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "dfcf4eb9f42f81729729aed89eabeac668d2d9675ff19e21733d40eea83e51c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
