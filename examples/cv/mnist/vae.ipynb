{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST VAE\n",
    "\n",
    "The MNIST dataset is a dataset of handwritten digits that is commonly used as the 'Hello World' dataset in Deep Learning domain. It contains 60,000 training images and 10,000 testing images, and\n",
    "`carefree-learn` provided a straightforward API to access it.\n",
    "\n",
    "MNIST dataset can be used for training various image processing systems. In this article, we will focus on how to build our custom models to solve the Variational Auto Encoder (VAE) task on MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20c148231b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preparations\n",
    "\n",
    "import torch\n",
    "import cflearn\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "from typing import Optional\n",
    "from cflearn.types import losses_type\n",
    "from cflearn.types import tensor_dict_type\n",
    "from cflearn.protocol import TrainerState\n",
    "from cflearn.misc.toolkit import check_is_ci\n",
    "from cflearn.misc.toolkit import interpolate\n",
    "from cflearn.misc.toolkit import inject_debug\n",
    "from cflearn.modules.blocks import Lambda\n",
    "from cflearn.modules.blocks import UpsampleConv2d\n",
    "\n",
    "# MNIST dataset could be prepared with this one line of code\n",
    "data = cflearn.cv.MNISTData(batch_size=16, transform=\"for_generation\")\n",
    "\n",
    "# for reproduction\n",
    "np.random.seed(142857)\n",
    "torch.manual_seed(142857)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the MNIST dataset could be easily turned into a `DLDataModule` instance, which is the common data interface used in `carefree-learn`.\n",
    "\n",
    "> The `transform` argument specifies which transform do we want to use to pre-process the input batch. See [`Transforms`](https://carefree0910.me/carefree-learn-doc/docs/user-guides/computer-vision#transforms) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "\n",
    "For demo purpose, we are going to build a simple convolution-based VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cflearn.register_module(\"simple_vae\")\n",
    "class SimpleVAE(nn.Module):\n",
    "    def __init__(self, in_channels: int, img_size: int):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(1),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            Lambda(lambda t: t.view(-1, 4, 4, 4), name=\"reshape\"),\n",
    "            nn.Conv2d(4, 128, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            UpsampleConv2d(128, 64, kernel_size=3, padding=1, factor=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(64),\n",
    "            UpsampleConv2d(64, 32, kernel_size=3, padding=1, factor=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            UpsampleConv2d(32, in_channels, kernel_size=3, padding=1, factor=2),\n",
    "            Lambda(lambda t: interpolate(t, size=img_size, mode=\"bilinear\")),\n",
    "        )\n",
    "\n",
    "    def forward(self, net: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        net = self.encoder(net)\n",
    "        mu, log_var = net.chunk(2, dim=1)\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        net = eps * std + mu\n",
    "        net = self.decoder(net)\n",
    "        return {\"mu\": mu, \"log_var\": log_var, cflearn.PREDICTIONS_KEY: net}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few details that worth to be mentioned:\n",
    "+ We leveraged the [`register_module`](https://carefree0910.me/carefree-learn-doc/docs/developer-guides/computer-vision-customization#customize-models) API here, which can turn a general `nn.Module` instance to a [`ModelProtocol`](https://carefree0910.me/carefree-learn-doc/docs/design-principles/#model) in `carefree-learn`. After registered, it can be easily accessed with its name (`\"simple_vae\"`)\n",
    "+ We leveraged some built-in [common blocks](https://carefree0910.me/carefree-learn-doc/docs/design-principles#common-blocks) of `carefree-learn` to build our simple VAE:\n",
    "  + `Lambda`, which can turn a function to an `nn.Module`.\n",
    "  + `UpsampleConv2d`, which can be used to upsample the input image.\n",
    "  + `interpolate`, which is a handy function to resize the input image to the desired size.\n",
    "\n",
    "After we finished implementing our model, we need to implement the special loss used in VAE tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cflearn.register_loss_module(\"simple_vae\")\n",
    "@cflearn.register_loss_module(\"simple_vae_foo\")\n",
    "class SimpleVAELoss(cflearn.LossModule):\n",
    "    def forward(\n",
    "        self,\n",
    "        forward_results: tensor_dict_type,\n",
    "        batch: tensor_dict_type,\n",
    "        state: Optional[TrainerState] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> losses_type:\n",
    "        # reconstruction loss\n",
    "        original = batch[cflearn.INPUT_KEY]\n",
    "        reconstruction = forward_results[cflearn.PREDICTIONS_KEY]\n",
    "        mse = F.mse_loss(reconstruction, original)\n",
    "        # kld loss\n",
    "        mu = forward_results[\"mu\"]\n",
    "        log_var = forward_results[\"log_var\"]\n",
    "        kld_losses = -0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim=1)\n",
    "        kld_loss = torch.mean(kld_losses, dim=0)\n",
    "        # gather\n",
    "        loss = mse + 0.001 * kld_loss\n",
    "        return {\"mse\": mse, \"kld\": kld_loss, cflearn.LOSS_KEY: loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We used `register_loss_module` to register a general `LossModule` instance to a `LossProtocol` in `carefree-learn`.\n",
    "+ We can call `register_loss_module` multiple times to assign multiple names to the same loss function.\n",
    "+ When the loss function shares the same name with the model, we don't need to specify the `loss_name` argument explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "_                                                                                                                 \n",
      "  SimpleVAE                                                                                                       \n",
      "    Sequential-0                     [-1, 1, 28, 28]                                [-1, 128]               97,376\n",
      "      Conv2d-0                       [-1, 1, 28, 28]                         [-1, 16, 28, 28]                  160\n",
      "      ReLU-0                        [-1, 16, 28, 28]                         [-1, 16, 28, 28]                    0\n",
      "      BatchNorm2d-0                 [-1, 16, 28, 28]                         [-1, 16, 28, 28]                   32\n",
      "      MaxPool2d-0                   [-1, 16, 28, 28]                         [-1, 16, 14, 14]                    0\n",
      "      Conv2d-1                      [-1, 16, 14, 14]                         [-1, 32, 14, 14]                4,640\n",
      "      ReLU-1                        [-1, 32, 14, 14]                         [-1, 32, 14, 14]                    0\n",
      "      BatchNorm2d-1                 [-1, 32, 14, 14]                         [-1, 32, 14, 14]                   64\n",
      "      MaxPool2d-1                   [-1, 32, 14, 14]                           [-1, 32, 7, 7]                    0\n",
      "      Conv2d-2                        [-1, 32, 7, 7]                           [-1, 64, 7, 7]               18,496\n",
      "      ReLU-2                          [-1, 64, 7, 7]                           [-1, 64, 7, 7]                    0\n",
      "      BatchNorm2d-2                   [-1, 64, 7, 7]                           [-1, 64, 7, 7]                  128\n",
      "      MaxPool2d-2                     [-1, 64, 7, 7]                           [-1, 64, 3, 3]                    0\n",
      "      Conv2d-3                        [-1, 64, 3, 3]                          [-1, 128, 3, 3]               73,856\n",
      "      ReLU-3                         [-1, 128, 3, 3]                          [-1, 128, 3, 3]                    0\n",
      "      AdaptiveAvgPool2d              [-1, 128, 3, 3]                          [-1, 128, 1, 1]                    0\n",
      "      Flatten                        [-1, 128, 1, 1]                                [-1, 128]                    0\n",
      "    Sequential-1                            [-1, 64]                          [-1, 1, 28, 28]               93,633\n",
      "      Lambda-0                              [-1, 64]                            [-1, 4, 4, 4]                    0\n",
      "      Conv2d                           [-1, 4, 4, 4]                          [-1, 128, 4, 4]                  640\n",
      "      ReLU-0                         [-1, 128, 4, 4]                          [-1, 128, 4, 4]                    0\n",
      "      BatchNorm2d-0                  [-1, 128, 4, 4]                          [-1, 128, 4, 4]                  256\n",
      "      UpsampleConv2d-0               [-1, 128, 4, 4]                           [-1, 64, 8, 8]               73,792\n",
      "        Interpolate                  [-1, 128, 4, 4]                          [-1, 128, 8, 8]                    0\n",
      "      ReLU-1                          [-1, 64, 8, 8]                           [-1, 64, 8, 8]                    0\n",
      "      BatchNorm2d-1                   [-1, 64, 8, 8]                           [-1, 64, 8, 8]                  128\n",
      "      UpsampleConv2d-1                [-1, 64, 8, 8]                         [-1, 32, 16, 16]               18,464\n",
      "        Interpolate                   [-1, 64, 8, 8]                         [-1, 64, 16, 16]                    0\n",
      "      ReLU-2                        [-1, 32, 16, 16]                         [-1, 32, 16, 16]                    0\n",
      "      BatchNorm2d-2                 [-1, 32, 16, 16]                         [-1, 32, 16, 16]                   64\n",
      "      UpsampleConv2d-2              [-1, 32, 16, 16]                          [-1, 1, 32, 32]                  289\n",
      "        Interpolate                 [-1, 32, 16, 16]                         [-1, 32, 32, 32]                    0\n",
      "      Lambda-1                       [-1, 1, 32, 32]                          [-1, 1, 28, 28]                    0\n",
      "========================================================================================================================\n",
      "Total params: 191,009\n",
      "Trainable params: 191,009\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.35\n",
      "Params size (MB): 0.73\n",
      "Estimated Total Size (MB): 2.08\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "| epoch  1   [1000 / 3750] [14.901s] | kld : 32.94150 | loss : 0.256339 | mse : 0.223398 | score : -0.25633 |\n",
      "| epoch  1   [2000 / 3750] [12.724s] | kld : 37.33559 | loss : 0.230748 | mse : 0.193413 | score : -0.23074 |\n",
      "| epoch  1   [3000 / 3750] [12.693s] | kld : 38.65885 | loss : 0.217656 | mse : 0.178998 | score : -0.21765 |\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      ">  [ info ] restoring from _logs\\2021-10-27_16-35-57-473557\\checkpoints\\model_3000.pt\n",
      "| epoch  -1  [  -1 / 3750] [10.248s] | kld : 38.65885 | loss : 0.218077 | mse : 0.179418 | score : -0.21807 |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cflearn.api.cv.pipeline.CarefreePipeline at 0x20c1468b9e8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice that we don't need to explicitly specify `loss_name`!\n",
    "cflearn.api.fit_cv(\n",
    "    data,\n",
    "    \"simple_vae\",\n",
    "    {\"in_channels\": 1, \"img_size\": 28},\n",
    "    fixed_epoch=1,                                  # for demo purpose, we only train our model for 1 epoch\n",
    "    cuda=0 if torch.cuda.is_available() else None,  # use CUDA if possible\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can still specify `loss_name` explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "Layer (type)                             Input Shape                             Output Shape    Trainable Param #\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "_                                                                                                                 \n",
      "  SimpleVAE                                                                                                       \n",
      "    Sequential-0                     [-1, 1, 28, 28]                                [-1, 128]               97,376\n",
      "      Conv2d-0                       [-1, 1, 28, 28]                         [-1, 16, 28, 28]                  160\n",
      "      ReLU-0                        [-1, 16, 28, 28]                         [-1, 16, 28, 28]                    0\n",
      "      BatchNorm2d-0                 [-1, 16, 28, 28]                         [-1, 16, 28, 28]                   32\n",
      "      MaxPool2d-0                   [-1, 16, 28, 28]                         [-1, 16, 14, 14]                    0\n",
      "      Conv2d-1                      [-1, 16, 14, 14]                         [-1, 32, 14, 14]                4,640\n",
      "      ReLU-1                        [-1, 32, 14, 14]                         [-1, 32, 14, 14]                    0\n",
      "      BatchNorm2d-1                 [-1, 32, 14, 14]                         [-1, 32, 14, 14]                   64\n",
      "      MaxPool2d-1                   [-1, 32, 14, 14]                           [-1, 32, 7, 7]                    0\n",
      "      Conv2d-2                        [-1, 32, 7, 7]                           [-1, 64, 7, 7]               18,496\n",
      "      ReLU-2                          [-1, 64, 7, 7]                           [-1, 64, 7, 7]                    0\n",
      "      BatchNorm2d-2                   [-1, 64, 7, 7]                           [-1, 64, 7, 7]                  128\n",
      "      MaxPool2d-2                     [-1, 64, 7, 7]                           [-1, 64, 3, 3]                    0\n",
      "      Conv2d-3                        [-1, 64, 3, 3]                          [-1, 128, 3, 3]               73,856\n",
      "      ReLU-3                         [-1, 128, 3, 3]                          [-1, 128, 3, 3]                    0\n",
      "      AdaptiveAvgPool2d              [-1, 128, 3, 3]                          [-1, 128, 1, 1]                    0\n",
      "      Flatten                        [-1, 128, 1, 1]                                [-1, 128]                    0\n",
      "    Sequential-1                            [-1, 64]                          [-1, 1, 28, 28]               93,633\n",
      "      Lambda-0                              [-1, 64]                            [-1, 4, 4, 4]                    0\n",
      "      Conv2d                           [-1, 4, 4, 4]                          [-1, 128, 4, 4]                  640\n",
      "      ReLU-0                         [-1, 128, 4, 4]                          [-1, 128, 4, 4]                    0\n",
      "      BatchNorm2d-0                  [-1, 128, 4, 4]                          [-1, 128, 4, 4]                  256\n",
      "      UpsampleConv2d-0               [-1, 128, 4, 4]                           [-1, 64, 8, 8]               73,792\n",
      "        Interpolate                  [-1, 128, 4, 4]                          [-1, 128, 8, 8]                    0\n",
      "      ReLU-1                          [-1, 64, 8, 8]                           [-1, 64, 8, 8]                    0\n",
      "      BatchNorm2d-1                   [-1, 64, 8, 8]                           [-1, 64, 8, 8]                  128\n",
      "      UpsampleConv2d-1                [-1, 64, 8, 8]                         [-1, 32, 16, 16]               18,464\n",
      "        Interpolate                   [-1, 64, 8, 8]                         [-1, 64, 16, 16]                    0\n",
      "      ReLU-2                        [-1, 32, 16, 16]                         [-1, 32, 16, 16]                    0\n",
      "      BatchNorm2d-2                 [-1, 32, 16, 16]                         [-1, 32, 16, 16]                   64\n",
      "      UpsampleConv2d-2              [-1, 32, 16, 16]                          [-1, 1, 32, 32]                  289\n",
      "        Interpolate                 [-1, 32, 16, 16]                         [-1, 32, 32, 32]                    0\n",
      "      Lambda-1                       [-1, 1, 32, 32]                          [-1, 1, 28, 28]                    0\n",
      "========================================================================================================================\n",
      "Total params: 191,009\n",
      "Trainable params: 191,009\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.35\n",
      "Params size (MB): 0.73\n",
      "Estimated Total Size (MB): 2.08\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      ">  [ info ] entered training loop\n",
      "| epoch  1   [1000 / 3750] [12.829s] | kld : 30.39608 | loss : 0.258902 | mse : 0.228506 | score : -0.25890 |\n",
      "| epoch  1   [2000 / 3750] [12.772s] | kld : 34.63805 | loss : 0.228807 | mse : 0.194169 | score : -0.22880 |\n",
      "| epoch  1   [3000 / 3750] [12.860s] | kld : 40.67327 | loss : 0.216177 | mse : 0.175504 | score : -0.21617 |\n",
      ">  [ info ] rolling back to the best checkpoint\n",
      ">  [ info ] restoring from _logs\\2021-10-27_16-36-48-116065\\checkpoints\\model_3000.pt\n",
      "| epoch  -1  [  -1 / 3750] [10.765s] | kld : 40.67327 | loss : 0.215523 | mse : 0.174849 | score : -0.21552 |\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cflearn.api.cv.pipeline.CarefreePipeline at 0x20c0d5d9e80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cflearn.api.fit_cv(\n",
    "    data,\n",
    "    \"simple_vae\",\n",
    "    {\"in_channels\": 1, \"img_size\": 28},\n",
    "    loss_name=\"simple_vae_foo\",                     # we used the second registered name here\n",
    "    fixed_epoch=1,                                  # for demo purpose, we only train our model for 1 epoch\n",
    "    cuda=0 if torch.cuda.is_available() else None,  # use CUDA if possible\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
