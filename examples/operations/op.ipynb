{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations\n",
    "\n",
    "`Operations` are toy datasets for us to illustrate how to build your own models in `carefree-learn`. We will generate some artificial datasets based on basic *operations*, namely `sum`, `prod` and their mixture, to deonstrate the validity of our customized model.\n",
    "\n",
    "Here are the formula of the definitions of the datasets:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{D}_{\\text {sum}} &=\\{(\\mathbf x,\\sum_{i=1}^d x_i)|\\mathbf x\\in\\mathbb{R}^d\\} \\\\\n",
    "\\mathcal{D}_{\\text {prod}}&=\\{(\\mathbf x,\\prod_{i=1}^d x_i)|\\mathbf x\\in\\mathbb{R}^d\\} \\\\\n",
    "\\mathcal{D}_{\\text {mix}} &=\\{(\\mathbf x,[y_{\\text{sum}},y_{\\text{prod}}])|\\mathbf x\\in\\mathbb{R}^d\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In short, the `sum` dataset simply sums up the features, the `prod` dataset simply multiplies all the features, and the `mix` dataset is a mixture of `add` and `prod`. Here are the codes to generate them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cflearn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "from cflearn.modules.blocks import Linear\n",
    "\n",
    "# for reproduction\n",
    "np.random.seed(142857)\n",
    "torch.manual_seed(142857)\n",
    "\n",
    "# prepare\n",
    "dim = 5\n",
    "num_data = 10000\n",
    "\n",
    "x = np.random.random([num_data, dim]) * 2.0\n",
    "y_add = np.sum(x, axis=1, keepdims=True)\n",
    "y_prod = np.prod(x, axis=1, keepdims=True)\n",
    "y_mix = np.hstack([y_add, y_prod])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to hold the datasets' property, we should not apply any pre-processing strategies to these datasets. Fortunately, `carefree-learn` has provided a simple configuration for us to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `reg` represents a regression task\n",
    "# `use_simplify_data` indicates that `carefree-learn` will do nothing to the input data\n",
    "kwargs = {\"task_type\": \"reg\", \"use_simplify_data\": True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `add` Dataset\n",
    "\n",
    "It's pretty clear that the `add` dataset could be solved easily with a `linear` model\n",
    "\n",
    "$$\n",
    "\\hat y = wx + b,\\quad w\\in\\mathbb{R}^{1\\times d},b\\in\\mathbb{R}^{1\\times 1}\n",
    "$$\n",
    "\n",
    "because the *ground truth* of `add` dataset could be represented as `linear` model, where\n",
    "\n",
    "$$\n",
    "w=[1,1,...,1],b=[0]\n",
    "$$\n",
    "\n",
    "Although this is a simple task, using Neural Networks to solve it might actually fail because it is likely to overfit the training set with some strange representation. We can demonstrate it by lifting a simple, quick experiment with the help of `carefree-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add\n",
    "linear = cflearn.make(\"linear\", **kwargs).fit(x, y_add)\n",
    "fcnn = cflearn.make(\"fcnn\", **kwargs).fit(x, y_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can evaluate the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~  [ info ] Results\n",
      "================================================================================================================================\n",
      "|        metrics         |                       mae                        |                       mse                        |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|          fcnn          |    0.346006    | -- 0.000000 -- |    -0.34600    |    0.154638    | -- 0.000000 -- |    -0.15463    |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|         linear         | -- 0.000418 -- | -- 0.000000 -- | -- -0.00041 -- | -- 0.000000 -- | -- 0.000000 -- | -- -0.00000 -- |\n",
      "================================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cftool.ml.utils.Comparer at 0x1c664c75ac8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cflearn.evaluate(x, y_add, pipelines=[linear, fcnn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, the `fcnn` (Fully Connected Neural Network) model actually fails to reach a satisfying result, while the `linear` model approaches to the ground truth easily.\n",
    "\n",
    "We can also check whether the model has *actually* learned the ground truth by checking its parameters ($w$ and $b$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: tensor([[0.9998, 0.9999, 1.0002, 0.9997, 1.0000]], device='cuda:0')\n",
      "b: tensor([-0.0001], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "linear_core = linear.model.heads[\"linear\"].linear\n",
    "print(f\"w: {linear_core.weight.data}\")\n",
    "print(f\"b: {linear_core.bias.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not perfect, but we are happy enoughðŸ˜†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `prod` Dataset\n",
    "\n",
    "However, when it comes to the `prod` dataset, the `linear` model is likely to face the *underfitting* issue because theoratically it cannot represent such formulation:\n",
    "\n",
    "$$\n",
    "y=\\prod_{i=1}^{d}x_i\n",
    "$$\n",
    "\n",
    "Neural Networks, on the other side, are able to represent **ANY** functions ([Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)). In this case, the `fcnn` model should be able to outperform the `linear` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linear = cflearn.make(\"linear\", **kwargs).fit(x, y_prod)\n",
    "fcnn = cflearn.make(\"fcnn\", **kwargs).fit(x, y_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can evaluate the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~  [ info ] Results\n",
      "================================================================================================================================\n",
      "|        metrics         |                       mae                        |                       mse                        |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|          fcnn          | -- 0.167043 -- | -- 0.000000 -- | -- -0.16704 -- | -- 0.152063 -- | -- 0.000000 -- | -- -0.15206 -- |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|         linear         |    0.955200    | -- 0.000000 -- |    -0.95520    |    2.798824    | -- 0.000000 -- |    -2.79882    |\n",
      "================================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cftool.ml.utils.Comparer at 0x1c664c4b860>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cflearn.evaluate(x, y_prod, pipelines=[linear, fcnn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although `fcnn` outperforms `linear`, it is still not as satisfied as the results that we've got in `add` dataset. That's because although `fcnn` has strong approximation power, its representations are basically based on the `add` operations between features, and the non-linearities come from an activation function applied to **EACH** neuron. Which means, `fcnn` can hardly learn `prod` operation **ACROSS** features.\n",
    "\n",
    "A trivial thought is to manually `extract` the `prod` features $\\tilde x$ from the input data:\n",
    "\n",
    "$$\n",
    "\\tilde x\\triangleq \\prod_{i=1}^d x_i\n",
    "$$\n",
    "\n",
    "After which a `linear` model should solve the problem, because the *ground truth* here is simply\n",
    "\n",
    "$$\n",
    "w=[1],b=[0]\n",
    "$$\n",
    "\n",
    "But how could we apply this prior knowledge to our model? Thanks to `carefree-learn`, this is actually quite simple with only a few lines of codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register an `extract` which represents the `prod` operation\n",
    "@cflearn.register_extractor(\"prod_extractor\")\n",
    "class ProdExtractor(cflearn.ExtractorBase):\n",
    "    @property\n",
    "    def out_dim(self) -> int:\n",
    "        return 1\n",
    "\n",
    "    def forward(self, net: torch.Tensor) -> torch.Tensor:\n",
    "        return net.prod(dim=1, keepdim=True)\n",
    "\n",
    "\n",
    "# define the `Config` for this `extract`\n",
    "# since `ProdExtractor` don't need any configurations, we can simply return an empty dict here\n",
    "@cflearn.register_config(\"prod_extractor\", \"default\")\n",
    "class ProdExtractorConfig(cflearn.Configs):\n",
    "    def get_default(self) -> Dict[str, Any]:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you are interested in how does `extract` actually work in `carefree-learn`, please refer to [pipe](https://carefree0910.me/carefree-learn-doc/docs/design-principles#pipe) and [extract](https://carefree0910.me/carefree-learn-doc/docs/design-principles#extract) for more information.\n",
    "\n",
    "After defining the `extract`, we need to define a model that leverages it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we call this new model `prod`\n",
    "@cflearn.register_model(\"prod\")\n",
    "# we use our new `extract` followed by traditional `linear` model\n",
    "@cflearn.register_pipe(\"linear\", extractor=\"prod_extractor\")\n",
    "class ProdNetwork(cflearn.ModelBase):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prod = cflearn.make(\"prod\", **kwargs).fit(x, y_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can evaluate the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~  [ info ] Results\n",
      "================================================================================================================================\n",
      "|        metrics         |                       mae                        |                       mse                        |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|          fcnn          |    0.167043    | -- 0.000000 -- |    -0.16704    |    0.152063    | -- 0.000000 -- |    -0.15206    |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|         linear         |    0.955200    | -- 0.000000 -- |    -0.95520    |    2.798824    | -- 0.000000 -- |    -2.79882    |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|          prod          | -- 0.000143 -- | -- 0.000000 -- | -- -0.00014 -- | -- 0.000000 -- | -- 0.000000 -- | -- -0.00000 -- |\n",
      "================================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cftool.ml.utils.Comparer at 0x1c67f0a54a8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cflearn.evaluate(x, y_prod, pipelines=[linear, fcnn, prod])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, the `prod` approaches to the ground truth easily.\n",
    "\n",
    "We can also check whether the model has actually learned the ground truth by checking its parameters ($w$ and $b$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 1.000136, b: -0.000113\n"
     ]
    }
   ],
   "source": [
    "prod_linear = prod.model.heads[\"linear\"].linear\n",
    "print(f\"w: {prod_linear.weight.item():8.6f}, b: {prod_linear.bias.item():8.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not perfect, but we are happy enoughðŸ˜†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `mix` Dataset\n",
    "\n",
    "Now comes to the fun part: what if we mix up `add` and `prod` dataset? Since `linear` is professional in `add`, `prod` is professional in `prod`, and `fcnn` is **QUITE** professional in **ALL** datasets (ðŸ¤£), it is hard to tell which one will outshine in the `mix` dataset. So let's do an experiment to obtain an empirical conclusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "linear = cflearn.make(\"linear\", **kwargs).fit(x, y_mix)\n",
    "fcnn = cflearn.make(\"fcnn\", **kwargs).fit(x, y_mix)\n",
    "prod = cflearn.make(\"prod\", **kwargs).fit(x, y_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can evaluate the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~  [ info ] Results\n",
      "================================================================================================================================\n",
      "|        metrics         |                       mae                        |                       mse                        |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|          fcnn          | -- 0.158854 -- | -- 0.000000 -- | -- -0.15885 -- | -- 0.094512 -- | -- 0.000000 -- | -- -0.09451 -- |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|         linear         |    0.341526    | -- 0.000000 -- |    -0.34152    |    1.104427    | -- 0.000000 -- |    -1.10442    |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|          prod          |    0.341261    | -- 0.000000 -- |    -0.34126    |    0.435841    | -- 0.000000 -- |    -0.43584    |\n",
      "================================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cftool.ml.utils.Comparer at 0x1c664c75978>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cflearn.evaluate(x, y_mix, pipelines=[linear, fcnn, prod])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that the non-expert in both domain (`fcnn`) outperforms the domain experts (`linear`, `prod`)! But again, this is far from satisfying because theoratically we can combine the domain experts to build an expert in `mix` dataset.\n",
    "\n",
    "Thanks to `carefree-learn`, we again can actually do so, but this time we'll need some more coding. Recall that we build an expert in `prod` dataset by defining a novel `extract`, because we needed to pre-process the input data. However in `mix`, what we actually need is to combine `linear` and `prod`, which means we need to define a novel `head` this time.\n",
    "\n",
    "> If you are interested in how does `head` actually work in `carefree-learn`, please refer to [pipe](https://carefree0910.me/carefree-learn-doc/docs/design-principles#pipe) and [head](https://carefree0910.me/carefree-learn-doc/docs/design-principles#head) for more information.\n",
    "\n",
    "Concretely, suppose we already have two models, $f_1$ and $f_2$, that are experts in `add` dataset and `prod` dataset respectively. What we need to do is to combine the first dimension of $f_1(\\mathbf x)$ and the second dimension of $f_2(\\mathbf x)$ to construct our final outputs:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_1(\\mathbf x) \\triangleq [\\hat y_{11}, \\hat y_{12}] \\\\\n",
    "f_2(\\mathbf x) \\triangleq [\\hat y_{21}, \\hat y_{22}] \\\\\n",
    "\\Rightarrow \\tilde f(\\mathbf x) \\triangleq [\\hat y_{11}, \\hat y_{22}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since $\\hat y_{11}$ can fit `add` dataset perfectly, $\\hat y_{22}$ can fit `prod` dataset perfectly, $\\tilde f(\\mathbf x)$ should be able to fit `mix` dataset perfectly. Let's implement this model to demonstrate it with experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@cflearn.register_head(\"mixture_head\")\n",
    "class MixtureHead(cflearn.HeadBase):\n",
    "    def __init__(self, in_dim: int, out_dim: int, target_dim: int):\n",
    "        super().__init__()\n",
    "        # when `target_dim == 0`, it represents an `add` head (y_11)\n",
    "        # when `target_dim == 1`, it represents a `prod` head (y_22)\n",
    "        self.dim = target_dim\n",
    "        self.linear = Linear(in_dim, 1)\n",
    "\n",
    "    def forward(self, net: torch.Tensor) -> torch.Tensor:\n",
    "        target = self.linear(net)\n",
    "        zeros = torch.zeros_like(target)\n",
    "        tensors = [target, zeros] if self.dim == 0 else [zeros, target]\n",
    "        return torch.cat(tensors, dim=1)\n",
    "\n",
    "# we need to define two configurations for `add` and `prod` respectively    \n",
    "\n",
    "@cflearn.register_head_config(\"mixture_head\", \"add\")\n",
    "class MixtureHeadAddConfig(cflearn.HeadConfigs):\n",
    "    def get_default(self) -> Dict[str, Any]:\n",
    "        return {\"target_dim\": 0}\n",
    "\n",
    "\n",
    "@cflearn.register_head_config(\"mixture_head\", \"prod\")\n",
    "class MixtureHeadProdConfig(cflearn.HeadConfigs):\n",
    "    def get_default(self) -> Dict[str, Any]:\n",
    "        return {\"target_dim\": 1}\n",
    "    \n",
    "# we use our new `head` to define the new model\n",
    "# note that we need two `pipe`s, one for `add` and the other for `prod`\n",
    "@cflearn.register_model(\"mixture\")\n",
    "@cflearn.register_pipe(\n",
    "    \"add\",\n",
    "    extractor=\"identity\",\n",
    "    head=\"mixture_head\",\n",
    "    head_config=\"add\",\n",
    ")\n",
    "@cflearn.register_pipe(\n",
    "    \"prod\",\n",
    "    extractor=\"prod_extractor\",\n",
    "    head=\"mixture_head\",\n",
    "    head_config=\"prod\",\n",
    ")\n",
    "class MixtureNetwork(cflearn.ModelBase):\n",
    "    pass\n",
    "\n",
    "mixture = cflearn.make(\"mixture\", **kwargs).fit(x, y_mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can evaluate the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~  [ info ] Results\n",
      "================================================================================================================================\n",
      "|        metrics         |                       mae                        |                       mse                        |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|          fcnn          |    0.158854    | -- 0.000000 -- |    -0.15885    |    0.094512    | -- 0.000000 -- |    -0.09451    |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|         linear         |    0.341526    | -- 0.000000 -- |    -0.34152    |    1.104427    | -- 0.000000 -- |    -1.10442    |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|        mixture         | -- 0.000219 -- | -- 0.000000 -- | -- -0.00021 -- | -- 0.000000 -- | -- 0.000000 -- | -- -0.00000 -- |\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "|          prod          |    0.341261    | -- 0.000000 -- |    -0.34126    |    0.435841    | -- 0.000000 -- |    -0.43584    |\n",
      "================================================================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cftool.ml.utils.Comparer at 0x1c69ffe8828>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cflearn.evaluate(x, y_mix, pipelines=[linear, fcnn, prod, mixture])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, the `mixture` approaches to the ground truth easily.\n",
    "\n",
    "We can also check whether the model has actually learned the ground truth by checking its parameters ($w$ and $b$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add  w: tensor([[1.0002, 0.9999, 1.0000, 1.0000, 0.9999]], device='cuda:0')\n",
      "add  b: tensor([-0.0002], device='cuda:0')\n",
      "prod w: tensor([[1.0001]], device='cuda:0')\n",
      "prod b: tensor([-0.0002], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "add_linear = mixture.model.heads[\"add\"].linear\n",
    "prod_linear = mixture.model.heads[\"prod\"].linear\n",
    "print(f\"add  w: {add_linear.weight.data}\")\n",
    "print(f\"add  b: {add_linear.bias.data}\")\n",
    "print(f\"prod w: {prod_linear.weight.data}\")\n",
    "print(f\"prod b: {prod_linear.bias.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not perfect, but we are happy enoughðŸ¥³\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "`Operations` are just artificial toy datasets, but quite handy for us to illustrate some basic concepts in `carefre-learn`. We hope that this small example can help you quickly walk through some development guides in `carefre-learn`, and help you leverage `carefree-learn` in your own tasks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
